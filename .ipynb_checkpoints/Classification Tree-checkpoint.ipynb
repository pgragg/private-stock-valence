{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from random import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score, explained_variance_score\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, RepeatVector, BatchNormalization, Convolution1D, Flatten, Lambda, Permute, MaxPooling1D, AlphaDropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path = 'features_by_valence/'\n",
    "label_set = ['low', 'high']\n",
    "low_files = glob.glob(root_path + label_set[0] + '/*')\n",
    "high_files = glob.glob(root_path + label_set[1] + '/*')\n",
    "\n",
    "features = {'low':[], 'high':[]}\n",
    "\n",
    "all_features = []\n",
    "\n",
    "for filename in low_files:\n",
    "    word_histogram = json.loads(open(filename).read())\n",
    "    features['low'].append(word_histogram)\n",
    "    all_features.append({'dictionary': word_histogram, 'label': 'low'})\n",
    "for filename in high_files:\n",
    "    word_histogram = json.loads(open(filename).read())\n",
    "    features['high'].append(word_histogram)\n",
    "    all_features.append({'dictionary': word_histogram, 'label': 'high'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 252\n",
      "train: 201\n",
      "test: 50\n"
     ]
    }
   ],
   "source": [
    "n_total = int(len(all_features))\n",
    "n_train = int(n_total * 0.8)\n",
    "n_test = int(n_total * 0.2)\n",
    "\n",
    "print('total: ' + str(n_total))\n",
    "print('train: ' + str(n_train))\n",
    "print('test: ' + str(n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "\n",
    "def word_list(percent_occurance):\n",
    "    key_set = {}\n",
    "\n",
    "    for feature_d in all_features:\n",
    "        keys = feature_d['dictionary'].keys()\n",
    "        for key in keys:\n",
    "            key_set[key] = key_set.get(key, 0) + 1\n",
    "    key_set\n",
    "    # Return words that occur in the top percent_occurance% of documents\n",
    "    key_set = dict([[key, key_set[key]] for key in key_set if (key_set[key]/float(n_total) > float(percent_occurance))])\n",
    "\n",
    "    words_sorted_alphabetically = list(key_set.keys())\n",
    "    words_sorted_alphabetically.sort()\n",
    "    return words_sorted_alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_index(reference, key):\n",
    "    try:\n",
    "        return reference.index(key)\n",
    "    except:\n",
    "        return len(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe = {}\n",
    "i_2_label = {}\n",
    "\n",
    "def one_hot_encode(reference, dictionary):\n",
    "    ohe = np.zeros(len(reference) + 1)\n",
    "    for key in dictionary.keys():\n",
    "        pos = get_index(reference, key)\n",
    "        ohe[pos] = dictionary[key]\n",
    "    norm_lookup = float(np.linalg.norm(ohe))\n",
    "    normalized_ohe = [float(item)/float(norm_lookup) for item in ohe]\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode(y):\n",
    "    if y == 'high':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def decode(y):\n",
    "    if y == 1:\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'low'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(af):\n",
    "    classifications = {'low':0, 'high':0}\n",
    "    correct_preds = 0\n",
    "    all_preds = 0\n",
    "    for d_item in af:\n",
    "        feature = d_item['ohe_feature']\n",
    "        label = d_item['ohe_label']\n",
    "        classification = clf.predict([feature])[0]\n",
    "        classification = decode(classification)\n",
    "        classifications[classification] = classifications[classification] + 1\n",
    "        if classification == label:\n",
    "            correct_preds = correct_preds + 1\n",
    "        all_preds = all_preds + 1\n",
    "    print(classifications)\n",
    "    return float(correct_preds)/float(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preds_and_true(clf, af, ohe_words):\n",
    "    correct_preds = 0\n",
    "    all_preds = 0\n",
    "    preds = []\n",
    "    true = []\n",
    "    for d_item in af:\n",
    "        feature = d_item['ohe_feature']\n",
    "        label = d_item['ohe_label']\n",
    "        classification = clf.predict([feature])[0]\n",
    "        preds.append(classification)\n",
    "        true.append( label )\n",
    "    return [preds, true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_features_and_words(all_features, word_threshold):\n",
    "    ohe_words = word_list(word_threshold)\n",
    "    \n",
    "    for i, f in enumerate(all_features):\n",
    "        ohe_feature = one_hot_encode(ohe_words, f['dictionary'])\n",
    "        ohe_label = encode(f['label'])\n",
    "        all_features[i]['ohe_feature'] = ohe_feature\n",
    "        all_features[i]['ohe_label'] = ohe_label\n",
    "    return [all_features, ohe_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(all_features, ohe_words):\n",
    "    rounds = []\n",
    "    for i in range(20):\n",
    "        shuffle(all_features)\n",
    "        \n",
    "        train_features = all_features[0:n_train]\n",
    "        test_features = all_features[n_train:n_total]\n",
    "    \n",
    "        X = [f['ohe_feature'] for f in train_features]\n",
    "        y = [f['ohe_label'] for f in train_features]\n",
    "        \n",
    "        clf = GradientBoostingClassifier(random_state=0, learning_rate=0.005)\n",
    "        clf.fit(X, y)\n",
    "        \n",
    "        preds, true = preds_and_true(clf, train_features, ohe_words)\n",
    "        train_precision = precision_score(true, preds)\n",
    "        train_acc = accuracy_score(true, preds)\n",
    "        tr_ev = explained_variance_score(true,preds)\n",
    "        \n",
    "        preds, true = preds_and_true(clf, test_features, ohe_words)\n",
    "        test_precision = precision_score(true, preds)\n",
    "        test_acc = accuracy_score(true, preds)\n",
    "        te_ev = explained_variance_score(true, preds)\n",
    "        \n",
    "        rounds.append({\n",
    "            'train_precision': train_precision, \n",
    "            'train_acc': train_acc, \n",
    "            'tr_ev': tr_ev,\n",
    "            'test_precision': test_precision,\n",
    "            'test_acc': test_acc, \n",
    "            'te_ev': te_ev\n",
    "            \n",
    "        })\n",
    "    totals = {}\n",
    "    output = {}\n",
    "    for ro in rounds: \n",
    "        for k in ro:\n",
    "            ro_total_key = 'total-' + k\n",
    "            totals[ro_total_key] = totals.get(ro_total_key, 0) + ro[k]\n",
    "    for k in totals:\n",
    "        ou_av_key = 'av-' + k\n",
    "        output[ou_av_key] = (totals[k]/float(len(rounds)))\n",
    "    return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'av-total-te_ev': -0.60559675360554177,\n",
       " 'av-total-test_acc': 0.53921568627450978,\n",
       " 'av-total-test_precision': 0.46470682095682098,\n",
       " 'av-total-tr_ev': 0.68441162543144385,\n",
       " 'av-total-train_acc': 0.91567164179104488,\n",
       " 'av-total-train_precision': 0.99180114076448278}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 0.25\n",
    "all_features, ohe_words = encode_features_and_words(all_features, n)\n",
    "model(all_features, ohe_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25.)\n",
      "\n",
      "{'train_precision': 1.0, 'train_acc': 0.92537313432835822, 'tr_ev': 0.71703853955375263, 'test_precision': 0.40000000000000002, 'test_acc': 0.50980392156862742, 'te_ev': -0.4104938271604941}\n"
     ]
    }
   ],
   "source": [
    "clf = GradientBoostingClassifier(random_state=0, learning_rate=0.005)\n",
    "clf.fit(X, y)\n",
    "\n",
    "preds, true = preds_and_true(clf, train_features, ohe_words)\n",
    "train_precision = precision_score(true, preds)\n",
    "train_acc = accuracy_score(true, preds)\n",
    "tr_ev = explained_variance_score(true,preds)\n",
    "\n",
    "preds, true = preds_and_true(clf, test_features, ohe_words)\n",
    "test_precision = precision_score(true, preds)\n",
    "test_acc = accuracy_score(true, preds)\n",
    "te_ev = explained_variance_score(true, preds)\n",
    "print(str(n) + '.)' + \"\\n\")\n",
    "print({'train_precision': train_precision, \n",
    "    'train_acc': train_acc, \n",
    "    'tr_ev': tr_ev,\n",
    "    'test_precision': test_precision,\n",
    "    'test_acc': test_acc, \n",
    "    'te_ev': te_ev})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2822"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n = 0.001\n",
    "len(word_list(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(all_features, n=0.25):\n",
    "    all_features, ohe_words = encode_features_and_words(all_features, n)\n",
    "\n",
    "    shuffle(all_features)\n",
    "\n",
    "    train_features = all_features[0:n_train]\n",
    "    test_features = all_features[n_train:n_total]\n",
    "\n",
    "    X = [f['ohe_feature'] for f in train_features]\n",
    "    y = [f['ohe_label'] for f in train_features]\n",
    "    \n",
    "    x_val = [f['ohe_feature'] for f in test_features]\n",
    "    y_val = [f['ohe_label'] for f in test_features]\n",
    "    return [X, y, x_val, y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.25 is a solid value for n. Lower values mean letting in larger amounts of words (features)\n",
    "# Once validation accuracy is 66.67%, save the weights. That's the max. \n",
    "n = 0.25\n",
    "X, y, x_val, y_val = get_train_test(all_features, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = len(word_list(n)) + 1\n",
    "\n",
    "m1 = Sequential([\n",
    "    BatchNormalization(input_shape=(shape,)),\n",
    "    Dense(60, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    BatchNormalization(),\n",
    "    Dense(30, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(1, activation='sigmoid')   \n",
    "])\n",
    "\n",
    "m1.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "sched = [[0.0001, 2], [0.001, 20], [0.01, 10], [0.1, 2], [0.5, 1], [0.1, 5], [0.01, 20], [0.001, 40], [0.0001, 40], [0.00005, 40]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/2\n",
      "201/201 [==============================] - 6s 31ms/step - loss: 0.9273 - acc: 0.4677 - val_loss: 2.2144 - val_acc: 0.5294\n",
      "Epoch 2/2\n",
      "201/201 [==============================] - 0s 537us/step - loss: 0.8328 - acc: 0.5224 - val_loss: 1.5095 - val_acc: 0.5294\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/20\n",
      "201/201 [==============================] - 0s 582us/step - loss: 0.8089 - acc: 0.5075 - val_loss: 1.2447 - val_acc: 0.5294\n",
      "Epoch 2/20\n",
      "201/201 [==============================] - 0s 542us/step - loss: 0.7648 - acc: 0.5423 - val_loss: 1.0973 - val_acc: 0.5490\n",
      "Epoch 3/20\n",
      "201/201 [==============================] - 0s 765us/step - loss: 0.7778 - acc: 0.5124 - val_loss: 1.0068 - val_acc: 0.5490\n",
      "Epoch 4/20\n",
      "201/201 [==============================] - 0s 732us/step - loss: 0.7234 - acc: 0.5871 - val_loss: 0.9351 - val_acc: 0.5686\n",
      "Epoch 5/20\n",
      "201/201 [==============================] - 0s 631us/step - loss: 0.7399 - acc: 0.5821 - val_loss: 0.8617 - val_acc: 0.5882\n",
      "Epoch 6/20\n",
      "201/201 [==============================] - 0s 420us/step - loss: 0.7473 - acc: 0.5622 - val_loss: 0.8046 - val_acc: 0.5882\n",
      "Epoch 7/20\n",
      "201/201 [==============================] - 0s 582us/step - loss: 0.7904 - acc: 0.5522 - val_loss: 0.7651 - val_acc: 0.5882\n",
      "Epoch 8/20\n",
      "201/201 [==============================] - 0s 795us/step - loss: 0.7040 - acc: 0.5970 - val_loss: 0.7463 - val_acc: 0.5490\n",
      "Epoch 9/20\n",
      "201/201 [==============================] - 0s 460us/step - loss: 0.7310 - acc: 0.5622 - val_loss: 0.7214 - val_acc: 0.5490\n",
      "Epoch 10/20\n",
      "201/201 [==============================] - 0s 464us/step - loss: 0.6812 - acc: 0.5821 - val_loss: 0.7072 - val_acc: 0.5490\n",
      "Epoch 11/20\n",
      "201/201 [==============================] - 0s 615us/step - loss: 0.7737 - acc: 0.5323 - val_loss: 0.6786 - val_acc: 0.5490\n",
      "Epoch 12/20\n",
      "201/201 [==============================] - 0s 479us/step - loss: 0.6986 - acc: 0.5622 - val_loss: 0.6668 - val_acc: 0.5686\n",
      "Epoch 13/20\n",
      "201/201 [==============================] - 0s 404us/step - loss: 0.7686 - acc: 0.5224 - val_loss: 0.6679 - val_acc: 0.5686\n",
      "Epoch 14/20\n",
      "201/201 [==============================] - 0s 569us/step - loss: 0.7753 - acc: 0.5473 - val_loss: 0.6718 - val_acc: 0.5686\n",
      "Epoch 15/20\n",
      "201/201 [==============================] - 0s 615us/step - loss: 0.7642 - acc: 0.5821 - val_loss: 0.6672 - val_acc: 0.5686\n",
      "Epoch 16/20\n",
      "201/201 [==============================] - 0s 491us/step - loss: 0.6642 - acc: 0.5871 - val_loss: 0.6561 - val_acc: 0.5882\n",
      "Epoch 17/20\n",
      "201/201 [==============================] - 0s 421us/step - loss: 0.7278 - acc: 0.5572 - val_loss: 0.6498 - val_acc: 0.5882\n",
      "Epoch 18/20\n",
      "201/201 [==============================] - 0s 619us/step - loss: 0.7615 - acc: 0.5423 - val_loss: 0.6439 - val_acc: 0.6078\n",
      "Epoch 19/20\n",
      "201/201 [==============================] - 0s 726us/step - loss: 0.6437 - acc: 0.6318 - val_loss: 0.6374 - val_acc: 0.6078\n",
      "Epoch 20/20\n",
      "201/201 [==============================] - 0s 593us/step - loss: 0.7618 - acc: 0.5174 - val_loss: 0.6350 - val_acc: 0.6078\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/10\n",
      "201/201 [==============================] - 0s 557us/step - loss: 0.6726 - acc: 0.6567 - val_loss: 0.6321 - val_acc: 0.5882\n",
      "Epoch 2/10\n",
      "201/201 [==============================] - 0s 568us/step - loss: 0.6683 - acc: 0.6269 - val_loss: 0.6250 - val_acc: 0.5882\n",
      "Epoch 3/10\n",
      "201/201 [==============================] - 0s 462us/step - loss: 0.7273 - acc: 0.5721 - val_loss: 0.6230 - val_acc: 0.5294\n",
      "Epoch 4/10\n",
      "201/201 [==============================] - 0s 397us/step - loss: 0.6834 - acc: 0.6418 - val_loss: 0.6204 - val_acc: 0.5294\n",
      "Epoch 5/10\n",
      "201/201 [==============================] - 0s 624us/step - loss: 0.6576 - acc: 0.6070 - val_loss: 0.6171 - val_acc: 0.5294\n",
      "Epoch 6/10\n",
      "201/201 [==============================] - 0s 614us/step - loss: 0.7020 - acc: 0.5821 - val_loss: 0.6163 - val_acc: 0.5686\n",
      "Epoch 7/10\n",
      "201/201 [==============================] - 0s 790us/step - loss: 0.6720 - acc: 0.5721 - val_loss: 0.6151 - val_acc: 0.6275\n",
      "Epoch 8/10\n",
      "201/201 [==============================] - 0s 425us/step - loss: 0.7144 - acc: 0.5721 - val_loss: 0.6139 - val_acc: 0.6275\n",
      "Epoch 9/10\n",
      "201/201 [==============================] - 0s 487us/step - loss: 0.6426 - acc: 0.6119 - val_loss: 0.6110 - val_acc: 0.6275\n",
      "Epoch 10/10\n",
      "201/201 [==============================] - 0s 662us/step - loss: 0.7008 - acc: 0.5473 - val_loss: 0.6094 - val_acc: 0.6275\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/2\n",
      "201/201 [==============================] - 0s 463us/step - loss: 0.6943 - acc: 0.5821 - val_loss: 0.6100 - val_acc: 0.6275\n",
      "Epoch 2/2\n",
      "201/201 [==============================] - 0s 418us/step - loss: 0.6765 - acc: 0.5871 - val_loss: 0.6095 - val_acc: 0.6275\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/1\n",
      "201/201 [==============================] - 0s 389us/step - loss: 0.7317 - acc: 0.5721 - val_loss: 0.6093 - val_acc: 0.6275\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/5\n",
      "201/201 [==============================] - 0s 612us/step - loss: 0.7239 - acc: 0.5920 - val_loss: 0.6092 - val_acc: 0.6275\n",
      "Epoch 2/5\n",
      "201/201 [==============================] - 0s 498us/step - loss: 0.6769 - acc: 0.5721 - val_loss: 0.6089 - val_acc: 0.6275\n",
      "Epoch 3/5\n",
      "201/201 [==============================] - 0s 616us/step - loss: 0.7011 - acc: 0.6020 - val_loss: 0.6102 - val_acc: 0.6275\n",
      "Epoch 4/5\n",
      "201/201 [==============================] - 0s 474us/step - loss: 0.6819 - acc: 0.6567 - val_loss: 0.6115 - val_acc: 0.6275\n",
      "Epoch 5/5\n",
      "201/201 [==============================] - 0s 592us/step - loss: 0.6518 - acc: 0.6418 - val_loss: 0.6104 - val_acc: 0.5882\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/20\n",
      "201/201 [==============================] - 0s 574us/step - loss: 0.6493 - acc: 0.6667 - val_loss: 0.6095 - val_acc: 0.5882\n",
      "Epoch 2/20\n",
      "201/201 [==============================] - 0s 539us/step - loss: 0.6644 - acc: 0.5522 - val_loss: 0.6082 - val_acc: 0.6078\n",
      "Epoch 3/20\n",
      "201/201 [==============================] - 0s 646us/step - loss: 0.6870 - acc: 0.5672 - val_loss: 0.6074 - val_acc: 0.6078\n",
      "Epoch 4/20\n",
      "201/201 [==============================] - 0s 448us/step - loss: 0.6330 - acc: 0.6567 - val_loss: 0.6074 - val_acc: 0.6471\n",
      "Epoch 5/20\n",
      "201/201 [==============================] - 0s 752us/step - loss: 0.6467 - acc: 0.5920 - val_loss: 0.6071 - val_acc: 0.6863\n",
      "Epoch 6/20\n",
      "201/201 [==============================] - 0s 530us/step - loss: 0.7243 - acc: 0.6070 - val_loss: 0.6062 - val_acc: 0.7059\n",
      "Epoch 7/20\n",
      "201/201 [==============================] - 0s 509us/step - loss: 0.6256 - acc: 0.6418 - val_loss: 0.6076 - val_acc: 0.7059\n",
      "Epoch 8/20\n",
      "201/201 [==============================] - 0s 616us/step - loss: 0.6472 - acc: 0.6318 - val_loss: 0.6079 - val_acc: 0.6863\n",
      "Epoch 9/20\n",
      "201/201 [==============================] - 0s 586us/step - loss: 0.6146 - acc: 0.6766 - val_loss: 0.6068 - val_acc: 0.6863\n",
      "Epoch 10/20\n",
      "201/201 [==============================] - 0s 691us/step - loss: 0.6601 - acc: 0.6070 - val_loss: 0.6080 - val_acc: 0.6863\n",
      "Epoch 11/20\n",
      "201/201 [==============================] - 0s 455us/step - loss: 0.6903 - acc: 0.6020 - val_loss: 0.6085 - val_acc: 0.6863\n",
      "Epoch 12/20\n",
      "201/201 [==============================] - 0s 493us/step - loss: 0.6055 - acc: 0.6716 - val_loss: 0.6089 - val_acc: 0.6863\n",
      "Epoch 13/20\n",
      "201/201 [==============================] - 0s 439us/step - loss: 0.6364 - acc: 0.6368 - val_loss: 0.6095 - val_acc: 0.6667\n",
      "Epoch 14/20\n",
      "201/201 [==============================] - 0s 436us/step - loss: 0.6849 - acc: 0.5920 - val_loss: 0.6088 - val_acc: 0.6667\n",
      "Epoch 15/20\n",
      "201/201 [==============================] - 0s 401us/step - loss: 0.6258 - acc: 0.6716 - val_loss: 0.6095 - val_acc: 0.6667\n",
      "Epoch 16/20\n",
      "201/201 [==============================] - 0s 511us/step - loss: 0.6343 - acc: 0.6468 - val_loss: 0.6103 - val_acc: 0.6667\n",
      "Epoch 17/20\n",
      "201/201 [==============================] - 0s 425us/step - loss: 0.6182 - acc: 0.6866 - val_loss: 0.6104 - val_acc: 0.6667\n",
      "Epoch 18/20\n",
      "201/201 [==============================] - 0s 386us/step - loss: 0.5773 - acc: 0.6766 - val_loss: 0.6111 - val_acc: 0.6667\n",
      "Epoch 19/20\n",
      "201/201 [==============================] - 0s 394us/step - loss: 0.6312 - acc: 0.5970 - val_loss: 0.6117 - val_acc: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "201/201 [==============================] - 0s 396us/step - loss: 0.6063 - acc: 0.6766 - val_loss: 0.6127 - val_acc: 0.6667\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/40\n",
      "201/201 [==============================] - 0s 398us/step - loss: 0.6265 - acc: 0.6119 - val_loss: 0.6125 - val_acc: 0.6667\n",
      "Epoch 2/40\n",
      "201/201 [==============================] - 0s 413us/step - loss: 0.6405 - acc: 0.5871 - val_loss: 0.6127 - val_acc: 0.6667\n",
      "Epoch 3/40\n",
      "201/201 [==============================] - 0s 518us/step - loss: 0.6183 - acc: 0.6567 - val_loss: 0.6121 - val_acc: 0.6667\n",
      "Epoch 4/40\n",
      "201/201 [==============================] - 0s 666us/step - loss: 0.6038 - acc: 0.6816 - val_loss: 0.6123 - val_acc: 0.6667\n",
      "Epoch 5/40\n",
      "201/201 [==============================] - 0s 483us/step - loss: 0.6177 - acc: 0.6517 - val_loss: 0.6142 - val_acc: 0.6667\n",
      "Epoch 6/40\n",
      "201/201 [==============================] - 0s 484us/step - loss: 0.6008 - acc: 0.6468 - val_loss: 0.6141 - val_acc: 0.6667\n",
      "Epoch 7/40\n",
      "201/201 [==============================] - 0s 435us/step - loss: 0.5825 - acc: 0.6965 - val_loss: 0.6152 - val_acc: 0.6863\n",
      "Epoch 8/40\n",
      "201/201 [==============================] - 0s 417us/step - loss: 0.6693 - acc: 0.6119 - val_loss: 0.6168 - val_acc: 0.6471\n",
      "Epoch 9/40\n",
      "201/201 [==============================] - 0s 409us/step - loss: 0.6438 - acc: 0.5871 - val_loss: 0.6187 - val_acc: 0.6471\n",
      "Epoch 10/40\n",
      "201/201 [==============================] - 0s 404us/step - loss: 0.6240 - acc: 0.6269 - val_loss: 0.6197 - val_acc: 0.6471\n",
      "Epoch 11/40\n",
      "201/201 [==============================] - 0s 408us/step - loss: 0.6686 - acc: 0.6269 - val_loss: 0.6226 - val_acc: 0.6863\n",
      "Epoch 12/40\n",
      "201/201 [==============================] - 0s 644us/step - loss: 0.5761 - acc: 0.6915 - val_loss: 0.6250 - val_acc: 0.6667\n",
      "Epoch 13/40\n",
      "201/201 [==============================] - 0s 544us/step - loss: 0.6319 - acc: 0.6567 - val_loss: 0.6246 - val_acc: 0.6667\n",
      "Epoch 14/40\n",
      "201/201 [==============================] - 0s 408us/step - loss: 0.6354 - acc: 0.6318 - val_loss: 0.6220 - val_acc: 0.6667\n",
      "Epoch 15/40\n",
      "201/201 [==============================] - 0s 451us/step - loss: 0.6274 - acc: 0.6269 - val_loss: 0.6195 - val_acc: 0.6863\n",
      "Epoch 16/40\n",
      "201/201 [==============================] - 0s 394us/step - loss: 0.6006 - acc: 0.6866 - val_loss: 0.6186 - val_acc: 0.6667\n",
      "Epoch 17/40\n",
      "201/201 [==============================] - 0s 563us/step - loss: 0.6651 - acc: 0.5821 - val_loss: 0.6171 - val_acc: 0.6667\n",
      "Epoch 18/40\n",
      "201/201 [==============================] - 0s 459us/step - loss: 0.6297 - acc: 0.6468 - val_loss: 0.6155 - val_acc: 0.6667\n",
      "Epoch 19/40\n",
      "201/201 [==============================] - 0s 452us/step - loss: 0.6392 - acc: 0.5970 - val_loss: 0.6144 - val_acc: 0.6667\n",
      "Epoch 20/40\n",
      "201/201 [==============================] - 0s 373us/step - loss: 0.6599 - acc: 0.6468 - val_loss: 0.6139 - val_acc: 0.6471\n",
      "Epoch 21/40\n",
      "201/201 [==============================] - 0s 403us/step - loss: 0.6347 - acc: 0.5970 - val_loss: 0.6122 - val_acc: 0.6275\n",
      "Epoch 22/40\n",
      "201/201 [==============================] - 0s 397us/step - loss: 0.6182 - acc: 0.6866 - val_loss: 0.6127 - val_acc: 0.6275\n",
      "Epoch 23/40\n",
      "201/201 [==============================] - 0s 459us/step - loss: 0.5824 - acc: 0.6915 - val_loss: 0.6124 - val_acc: 0.6471\n",
      "Epoch 24/40\n",
      "201/201 [==============================] - 0s 424us/step - loss: 0.6596 - acc: 0.6318 - val_loss: 0.6123 - val_acc: 0.6471\n",
      "Epoch 25/40\n",
      "201/201 [==============================] - 0s 433us/step - loss: 0.6741 - acc: 0.5721 - val_loss: 0.6122 - val_acc: 0.6471\n",
      "Epoch 26/40\n",
      "201/201 [==============================] - 0s 425us/step - loss: 0.5606 - acc: 0.6965 - val_loss: 0.6123 - val_acc: 0.6275\n",
      "Epoch 27/40\n",
      "201/201 [==============================] - 0s 404us/step - loss: 0.5835 - acc: 0.6617 - val_loss: 0.6127 - val_acc: 0.6275\n",
      "Epoch 28/40\n",
      "201/201 [==============================] - 0s 478us/step - loss: 0.5862 - acc: 0.6468 - val_loss: 0.6141 - val_acc: 0.6275\n",
      "Epoch 29/40\n",
      "201/201 [==============================] - 0s 592us/step - loss: 0.6053 - acc: 0.6617 - val_loss: 0.6148 - val_acc: 0.6275\n",
      "Epoch 30/40\n",
      "201/201 [==============================] - 0s 548us/step - loss: 0.6476 - acc: 0.6318 - val_loss: 0.6163 - val_acc: 0.6275\n",
      "Epoch 31/40\n",
      "201/201 [==============================] - 0s 497us/step - loss: 0.5769 - acc: 0.6915 - val_loss: 0.6164 - val_acc: 0.6275\n",
      "Epoch 32/40\n",
      "201/201 [==============================] - 0s 435us/step - loss: 0.5669 - acc: 0.6816 - val_loss: 0.6157 - val_acc: 0.6275\n",
      "Epoch 33/40\n",
      "201/201 [==============================] - 0s 590us/step - loss: 0.5449 - acc: 0.7413 - val_loss: 0.6157 - val_acc: 0.6275\n",
      "Epoch 34/40\n",
      "201/201 [==============================] - 0s 534us/step - loss: 0.6153 - acc: 0.6617 - val_loss: 0.6151 - val_acc: 0.6275\n",
      "Epoch 35/40\n",
      "201/201 [==============================] - 0s 578us/step - loss: 0.6372 - acc: 0.6418 - val_loss: 0.6147 - val_acc: 0.6275\n",
      "Epoch 36/40\n",
      "201/201 [==============================] - 0s 548us/step - loss: 0.6017 - acc: 0.6716 - val_loss: 0.6152 - val_acc: 0.6275\n",
      "Epoch 37/40\n",
      "201/201 [==============================] - 0s 364us/step - loss: 0.5711 - acc: 0.6965 - val_loss: 0.6154 - val_acc: 0.6471\n",
      "Epoch 38/40\n",
      "201/201 [==============================] - 0s 531us/step - loss: 0.5606 - acc: 0.7015 - val_loss: 0.6151 - val_acc: 0.6667\n",
      "Epoch 39/40\n",
      "201/201 [==============================] - 0s 714us/step - loss: 0.5363 - acc: 0.7313 - val_loss: 0.6149 - val_acc: 0.6667\n",
      "Epoch 40/40\n",
      "201/201 [==============================] - 0s 562us/step - loss: 0.5992 - acc: 0.6716 - val_loss: 0.6150 - val_acc: 0.6667\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/40\n",
      "201/201 [==============================] - 0s 420us/step - loss: 0.5912 - acc: 0.6866 - val_loss: 0.6139 - val_acc: 0.6667\n",
      "Epoch 2/40\n",
      "201/201 [==============================] - 0s 485us/step - loss: 0.5999 - acc: 0.6716 - val_loss: 0.6137 - val_acc: 0.6471\n",
      "Epoch 3/40\n",
      "201/201 [==============================] - 0s 621us/step - loss: 0.5851 - acc: 0.6567 - val_loss: 0.6141 - val_acc: 0.6471\n",
      "Epoch 4/40\n",
      "201/201 [==============================] - 0s 553us/step - loss: 0.5279 - acc: 0.7313 - val_loss: 0.6135 - val_acc: 0.6471\n",
      "Epoch 5/40\n",
      "201/201 [==============================] - 0s 582us/step - loss: 0.5855 - acc: 0.6667 - val_loss: 0.6130 - val_acc: 0.6471\n",
      "Epoch 6/40\n",
      "201/201 [==============================] - 0s 430us/step - loss: 0.5901 - acc: 0.6716 - val_loss: 0.6122 - val_acc: 0.6471\n",
      "Epoch 7/40\n",
      "201/201 [==============================] - 0s 391us/step - loss: 0.5044 - acc: 0.7761 - val_loss: 0.6119 - val_acc: 0.6275\n",
      "Epoch 8/40\n",
      "201/201 [==============================] - 0s 391us/step - loss: 0.6000 - acc: 0.6468 - val_loss: 0.6108 - val_acc: 0.6275\n",
      "Epoch 9/40\n",
      "201/201 [==============================] - 0s 674us/step - loss: 0.5873 - acc: 0.6716 - val_loss: 0.6121 - val_acc: 0.6275\n",
      "Epoch 10/40\n",
      "201/201 [==============================] - 0s 587us/step - loss: 0.5680 - acc: 0.7114 - val_loss: 0.6109 - val_acc: 0.6275\n",
      "Epoch 11/40\n",
      "201/201 [==============================] - 0s 452us/step - loss: 0.5636 - acc: 0.7114 - val_loss: 0.6114 - val_acc: 0.6275\n",
      "Epoch 12/40\n",
      "201/201 [==============================] - 0s 494us/step - loss: 0.5325 - acc: 0.7413 - val_loss: 0.6101 - val_acc: 0.6471\n",
      "Epoch 13/40\n",
      "201/201 [==============================] - 0s 561us/step - loss: 0.5334 - acc: 0.7264 - val_loss: 0.6111 - val_acc: 0.6471\n",
      "Epoch 14/40\n",
      "201/201 [==============================] - 0s 659us/step - loss: 0.5818 - acc: 0.6667 - val_loss: 0.6110 - val_acc: 0.6471\n",
      "Epoch 15/40\n",
      "201/201 [==============================] - 0s 645us/step - loss: 0.5716 - acc: 0.7114 - val_loss: 0.6113 - val_acc: 0.6471\n",
      "Epoch 16/40\n",
      "201/201 [==============================] - 0s 452us/step - loss: 0.6205 - acc: 0.6517 - val_loss: 0.6113 - val_acc: 0.6471\n",
      "Epoch 17/40\n",
      "201/201 [==============================] - 0s 432us/step - loss: 0.5848 - acc: 0.6667 - val_loss: 0.6110 - val_acc: 0.6471\n",
      "Epoch 18/40\n",
      "201/201 [==============================] - 0s 411us/step - loss: 0.5438 - acc: 0.7562 - val_loss: 0.6115 - val_acc: 0.6471\n",
      "Epoch 19/40\n",
      "201/201 [==============================] - 0s 405us/step - loss: 0.5052 - acc: 0.7363 - val_loss: 0.6128 - val_acc: 0.6471\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201/201 [==============================] - 0s 563us/step - loss: 0.5403 - acc: 0.7164 - val_loss: 0.6142 - val_acc: 0.6471\n",
      "Epoch 21/40\n",
      "201/201 [==============================] - 0s 609us/step - loss: 0.5407 - acc: 0.6866 - val_loss: 0.6153 - val_acc: 0.6275\n",
      "Epoch 22/40\n",
      "201/201 [==============================] - 0s 559us/step - loss: 0.5763 - acc: 0.6816 - val_loss: 0.6152 - val_acc: 0.6275\n",
      "Epoch 23/40\n",
      "201/201 [==============================] - 0s 546us/step - loss: 0.4904 - acc: 0.7313 - val_loss: 0.6161 - val_acc: 0.6078\n",
      "Epoch 24/40\n",
      "201/201 [==============================] - 0s 596us/step - loss: 0.5199 - acc: 0.7413 - val_loss: 0.6160 - val_acc: 0.6078\n",
      "Epoch 25/40\n",
      "201/201 [==============================] - 0s 624us/step - loss: 0.5409 - acc: 0.7313 - val_loss: 0.6171 - val_acc: 0.6078\n",
      "Epoch 26/40\n",
      "201/201 [==============================] - 0s 590us/step - loss: 0.5553 - acc: 0.7114 - val_loss: 0.6167 - val_acc: 0.6078\n",
      "Epoch 27/40\n",
      "201/201 [==============================] - 0s 544us/step - loss: 0.5366 - acc: 0.7662 - val_loss: 0.6182 - val_acc: 0.6471\n",
      "Epoch 28/40\n",
      "201/201 [==============================] - 0s 609us/step - loss: 0.5381 - acc: 0.7612 - val_loss: 0.6185 - val_acc: 0.6471\n",
      "Epoch 29/40\n",
      "201/201 [==============================] - 0s 580us/step - loss: 0.5577 - acc: 0.6965 - val_loss: 0.6187 - val_acc: 0.6667\n",
      "Epoch 30/40\n",
      "201/201 [==============================] - 0s 603us/step - loss: 0.5383 - acc: 0.7164 - val_loss: 0.6177 - val_acc: 0.6667\n",
      "Epoch 31/40\n",
      "201/201 [==============================] - 0s 439us/step - loss: 0.5363 - acc: 0.6716 - val_loss: 0.6171 - val_acc: 0.6667\n",
      "Epoch 32/40\n",
      "201/201 [==============================] - 0s 574us/step - loss: 0.5118 - acc: 0.7313 - val_loss: 0.6141 - val_acc: 0.6667\n",
      "Epoch 33/40\n",
      "201/201 [==============================] - 0s 545us/step - loss: 0.5540 - acc: 0.7164 - val_loss: 0.6130 - val_acc: 0.6471\n",
      "Epoch 34/40\n",
      "201/201 [==============================] - 0s 594us/step - loss: 0.5683 - acc: 0.6766 - val_loss: 0.6125 - val_acc: 0.6471\n",
      "Epoch 35/40\n",
      "201/201 [==============================] - 0s 620us/step - loss: 0.5389 - acc: 0.7114 - val_loss: 0.6123 - val_acc: 0.6471\n",
      "Epoch 36/40\n",
      "201/201 [==============================] - 0s 602us/step - loss: 0.5480 - acc: 0.7313 - val_loss: 0.6123 - val_acc: 0.6667\n",
      "Epoch 37/40\n",
      "201/201 [==============================] - 0s 577us/step - loss: 0.5453 - acc: 0.6866 - val_loss: 0.6134 - val_acc: 0.6471\n",
      "Epoch 38/40\n",
      "201/201 [==============================] - 0s 790us/step - loss: 0.5572 - acc: 0.6915 - val_loss: 0.6150 - val_acc: 0.6667\n",
      "Epoch 39/40\n",
      "201/201 [==============================] - 0s 585us/step - loss: 0.4910 - acc: 0.7463 - val_loss: 0.6151 - val_acc: 0.6667\n",
      "Epoch 40/40\n",
      "201/201 [==============================] - 0s 482us/step - loss: 0.6080 - acc: 0.6816 - val_loss: 0.6167 - val_acc: 0.6667\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/40\n",
      "201/201 [==============================] - 0s 479us/step - loss: 0.5091 - acc: 0.7413 - val_loss: 0.6163 - val_acc: 0.6667\n",
      "Epoch 2/40\n",
      "201/201 [==============================] - 0s 444us/step - loss: 0.5564 - acc: 0.7264 - val_loss: 0.6164 - val_acc: 0.6667\n",
      "Epoch 3/40\n",
      "201/201 [==============================] - 0s 457us/step - loss: 0.5145 - acc: 0.7512 - val_loss: 0.6162 - val_acc: 0.6667\n",
      "Epoch 4/40\n",
      "201/201 [==============================] - 0s 778us/step - loss: 0.5417 - acc: 0.7264 - val_loss: 0.6169 - val_acc: 0.6667\n",
      "Epoch 5/40\n",
      "201/201 [==============================] - 0s 500us/step - loss: 0.5412 - acc: 0.7065 - val_loss: 0.6166 - val_acc: 0.6667\n",
      "Epoch 6/40\n",
      "201/201 [==============================] - 0s 519us/step - loss: 0.5082 - acc: 0.7463 - val_loss: 0.6148 - val_acc: 0.6667\n",
      "Epoch 7/40\n",
      "201/201 [==============================] - 0s 445us/step - loss: 0.5108 - acc: 0.7214 - val_loss: 0.6134 - val_acc: 0.6471\n",
      "Epoch 8/40\n",
      "201/201 [==============================] - 0s 577us/step - loss: 0.5416 - acc: 0.6716 - val_loss: 0.6125 - val_acc: 0.6667\n",
      "Epoch 9/40\n",
      "201/201 [==============================] - 0s 676us/step - loss: 0.5375 - acc: 0.7214 - val_loss: 0.6113 - val_acc: 0.6471\n",
      "Epoch 10/40\n",
      "201/201 [==============================] - 0s 665us/step - loss: 0.5429 - acc: 0.7214 - val_loss: 0.6104 - val_acc: 0.6078\n",
      "Epoch 11/40\n",
      "201/201 [==============================] - 0s 460us/step - loss: 0.5682 - acc: 0.7264 - val_loss: 0.6104 - val_acc: 0.6078\n",
      "Epoch 12/40\n",
      "201/201 [==============================] - 0s 731us/step - loss: 0.5242 - acc: 0.7015 - val_loss: 0.6114 - val_acc: 0.6078\n",
      "Epoch 13/40\n",
      "201/201 [==============================] - 0s 675us/step - loss: 0.5207 - acc: 0.7363 - val_loss: 0.6105 - val_acc: 0.6078\n",
      "Epoch 14/40\n",
      "201/201 [==============================] - 0s 592us/step - loss: 0.5252 - acc: 0.7363 - val_loss: 0.6091 - val_acc: 0.6078\n",
      "Epoch 15/40\n",
      "201/201 [==============================] - 0s 633us/step - loss: 0.5236 - acc: 0.6965 - val_loss: 0.6090 - val_acc: 0.6275\n",
      "Epoch 16/40\n",
      "201/201 [==============================] - 0s 575us/step - loss: 0.5166 - acc: 0.7562 - val_loss: 0.6093 - val_acc: 0.6275\n",
      "Epoch 17/40\n",
      "201/201 [==============================] - 0s 477us/step - loss: 0.5360 - acc: 0.7313 - val_loss: 0.6089 - val_acc: 0.6275\n",
      "Epoch 18/40\n",
      "201/201 [==============================] - 0s 449us/step - loss: 0.5470 - acc: 0.7214 - val_loss: 0.6089 - val_acc: 0.6275\n",
      "Epoch 19/40\n",
      "201/201 [==============================] - 0s 468us/step - loss: 0.5535 - acc: 0.6667 - val_loss: 0.6102 - val_acc: 0.6471\n",
      "Epoch 20/40\n",
      "201/201 [==============================] - 0s 451us/step - loss: 0.5172 - acc: 0.7164 - val_loss: 0.6112 - val_acc: 0.6471\n",
      "Epoch 21/40\n",
      "201/201 [==============================] - 0s 874us/step - loss: 0.5099 - acc: 0.7363 - val_loss: 0.6116 - val_acc: 0.6471\n",
      "Epoch 22/40\n",
      "201/201 [==============================] - 0s 835us/step - loss: 0.5489 - acc: 0.7114 - val_loss: 0.6113 - val_acc: 0.6471\n",
      "Epoch 23/40\n",
      "201/201 [==============================] - 0s 457us/step - loss: 0.5221 - acc: 0.7164 - val_loss: 0.6097 - val_acc: 0.6667\n",
      "Epoch 24/40\n",
      "201/201 [==============================] - 0s 495us/step - loss: 0.4811 - acc: 0.7612 - val_loss: 0.6103 - val_acc: 0.6667\n",
      "Epoch 25/40\n",
      "201/201 [==============================] - 0s 501us/step - loss: 0.5233 - acc: 0.7214 - val_loss: 0.6107 - val_acc: 0.6471\n",
      "Epoch 26/40\n",
      "201/201 [==============================] - 0s 485us/step - loss: 0.5261 - acc: 0.7015 - val_loss: 0.6101 - val_acc: 0.6275\n",
      "Epoch 27/40\n",
      "201/201 [==============================] - 0s 468us/step - loss: 0.5116 - acc: 0.7065 - val_loss: 0.6080 - val_acc: 0.6275\n",
      "Epoch 28/40\n",
      "201/201 [==============================] - 0s 460us/step - loss: 0.5449 - acc: 0.7015 - val_loss: 0.6060 - val_acc: 0.6471\n",
      "Epoch 29/40\n",
      "201/201 [==============================] - 0s 433us/step - loss: 0.5501 - acc: 0.7264 - val_loss: 0.6047 - val_acc: 0.6471\n",
      "Epoch 30/40\n",
      "201/201 [==============================] - 0s 552us/step - loss: 0.5039 - acc: 0.7363 - val_loss: 0.6036 - val_acc: 0.6471\n",
      "Epoch 31/40\n",
      "201/201 [==============================] - 0s 655us/step - loss: 0.5352 - acc: 0.7114 - val_loss: 0.6036 - val_acc: 0.6667\n",
      "Epoch 32/40\n",
      "201/201 [==============================] - 0s 564us/step - loss: 0.5635 - acc: 0.6965 - val_loss: 0.6021 - val_acc: 0.6667\n",
      "Epoch 33/40\n",
      "201/201 [==============================] - 0s 442us/step - loss: 0.5015 - acc: 0.7214 - val_loss: 0.6011 - val_acc: 0.6667\n",
      "Epoch 34/40\n",
      "201/201 [==============================] - 0s 458us/step - loss: 0.4612 - acc: 0.8060 - val_loss: 0.6013 - val_acc: 0.6667\n",
      "Epoch 35/40\n",
      "201/201 [==============================] - 0s 436us/step - loss: 0.5393 - acc: 0.7264 - val_loss: 0.6011 - val_acc: 0.6667\n",
      "Epoch 36/40\n",
      "201/201 [==============================] - 0s 428us/step - loss: 0.4896 - acc: 0.7512 - val_loss: 0.6014 - val_acc: 0.6667\n",
      "Epoch 37/40\n",
      "201/201 [==============================] - 0s 455us/step - loss: 0.5083 - acc: 0.7512 - val_loss: 0.6016 - val_acc: 0.6667\n",
      "Epoch 38/40\n",
      "201/201 [==============================] - 0s 704us/step - loss: 0.4887 - acc: 0.7512 - val_loss: 0.6032 - val_acc: 0.6667\n",
      "Epoch 39/40\n",
      "201/201 [==============================] - 0s 597us/step - loss: 0.4981 - acc: 0.7612 - val_loss: 0.6028 - val_acc: 0.6667\n",
      "Epoch 40/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201/201 [==============================] - 0s 638us/step - loss: 0.5085 - acc: 0.7413 - val_loss: 0.6009 - val_acc: 0.6667\n"
     ]
    }
   ],
   "source": [
    "for info in sched:\n",
    "    lr, epochs = info\n",
    "    m1.optimizer.lr = lr\n",
    "    m1.fit(np.array(X), np.array(y), epochs=epochs,  batch_size=64, validation_data=(np.array(x_val), np.array(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
