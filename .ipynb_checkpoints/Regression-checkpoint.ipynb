{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from random import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score, explained_variance_score\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Dropout, RepeatVector, BatchNormalization, Convolution1D, Flatten, Lambda, Permute, MaxPooling1D, AlphaDropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path = 'features_by_valence/'\n",
    "label_set = ['low', 'high']\n",
    "low_files = glob.glob(root_path + label_set[0] + '/*')\n",
    "high_files = glob.glob(root_path + label_set[1] + '/*')\n",
    "\n",
    "all_features = []\n",
    "\n",
    "for filename in low_files:\n",
    "    word_histogram = json.loads(open(filename).read())\n",
    "    all_features.append({\n",
    "        'dictionary': word_histogram, \n",
    "        'filename': filename.split('/')[-1]})\n",
    "for filename in high_files:\n",
    "    word_histogram = json.loads(open(filename).read())\n",
    "    all_features.append({\n",
    "        'dictionary': word_histogram, \n",
    "        'filename': filename.split('/')[-1]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'stock_price_changes_by_ticker_and_date/'\n",
    "\n",
    "# filenames = [f['filename'] for f in all_features]\n",
    "for i, f in enumerate(all_features):\n",
    "    pc = float(open(root_path + f['filename']).read())\n",
    "    all_features[i]['percent_change'] = pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 252\n",
      "train: 201\n",
      "test: 50\n"
     ]
    }
   ],
   "source": [
    "n_total = int(len(all_features))\n",
    "n_train = int(n_total * 0.8)\n",
    "n_test = int(n_total * 0.2)\n",
    "\n",
    "print('total: ' + str(n_total))\n",
    "print('train: ' + str(n_train))\n",
    "print('test: ' + str(n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "\n",
    "def word_list(percent_occurance):\n",
    "    key_set = {}\n",
    "\n",
    "    for feature_d in all_features:\n",
    "        keys = feature_d['dictionary'].keys()\n",
    "        for key in keys:\n",
    "            key_set[key] = key_set.get(key, 0) + 1\n",
    "    key_set\n",
    "    # Return words that occur in the top percent_occurance% of documents\n",
    "    key_set = dict([[key, key_set[key]] for key in key_set if (key_set[key]/float(n_total) > float(percent_occurance))])\n",
    "\n",
    "    words_sorted_alphabetically = list(key_set.keys())\n",
    "    words_sorted_alphabetically.sort()\n",
    "    return words_sorted_alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_index(reference, key):\n",
    "    try:\n",
    "        return reference.index(key)\n",
    "    except:\n",
    "        return len(reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ohe = {}\n",
    "i_2_label = {}\n",
    "\n",
    "def one_hot_encode(reference, dictionary):\n",
    "    ohe = np.zeros(len(reference) + 1)\n",
    "    for key in dictionary.keys():\n",
    "        pos = get_index(reference, key)\n",
    "        ohe[pos] = dictionary[key]\n",
    "    norm_lookup = float(np.linalg.norm(ohe))\n",
    "    normalized_ohe = [float(item)/float(norm_lookup) for item in ohe]\n",
    "    return normalized_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preds_and_true(clf, af, ohe_words):\n",
    "    correct_preds = 0\n",
    "    all_preds = 0\n",
    "    preds = []\n",
    "    true = []\n",
    "    for d_item in af:\n",
    "        feature = d_item['ohe_feature']\n",
    "        label = d_item['percent_change']\n",
    "        classification = clf.predict([feature])[0]\n",
    "        preds.append(classification)\n",
    "        true.append( label )\n",
    "    return [preds, true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_features_and_words(all_features, word_threshold):\n",
    "    ohe_words = word_list(word_threshold)\n",
    "    \n",
    "    for i, f in enumerate(all_features):\n",
    "        ohe_feature = one_hot_encode(ohe_words, f['dictionary'])\n",
    "        all_features[i]['ohe_feature'] = ohe_feature\n",
    "    return [all_features, ohe_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2822"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n = 0.001\n",
    "len(word_list(0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_test(all_features, n=0.25):\n",
    "    all_features, ohe_words = encode_features_and_words(all_features, n)\n",
    "\n",
    "    shuffle(all_features)\n",
    "\n",
    "    train_features = all_features[0:n_train]\n",
    "    test_features = all_features[n_train:n_total]\n",
    "\n",
    "    X = [f['ohe_feature'] for f in train_features]\n",
    "    y = [f['percent_change'] for f in train_features]\n",
    "    \n",
    "    x_val = [f['ohe_feature'] for f in test_features]\n",
    "    y_val = [f['percent_change'] for f in test_features]\n",
    "    return [X, y, x_val, y_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0.25 is a solid value for n. Lower values mean letting in larger amounts of words (features)\n",
    "# Once validation accuracy is 66.67%, save the weights. That's the max. \n",
    "n = 0.25\n",
    "X, y, x_val, y_val = get_train_test(all_features, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53579222257765668"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(np.array(y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2823"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = len(word_list(n)) + 1\n",
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = len(word_list(n)) + 1\n",
    "\n",
    "m1 = Sequential([\n",
    "    BatchNormalization(input_shape=(shape,)),\n",
    "    Dense(40, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(20, activation='tanh'),\n",
    "    Dropout(0.5),\n",
    "    BatchNormalization(),\n",
    "    Dense(1)   \n",
    "])\n",
    "\n",
    "m1.compile(optimizer='rmsprop',\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])\n",
    "\n",
    "sched = [[0.0001, 2], [0.001, 20], [0.01, 2], [0.1, 2], [0.5, 1], [0.1, 5], [0.01, 20], [0.001, 40], [0.0001, 80], [0.00005, 120]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/2\n",
      "201/201 [==============================] - 1s 4ms/step - loss: 31.9824 - mean_absolute_error: 3.5793 - acc: 0.0050 - val_loss: 25.1894 - val_mean_absolute_error: 3.4366 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "201/201 [==============================] - 0s 275us/step - loss: 31.2742 - mean_absolute_error: 3.5028 - acc: 0.0050 - val_loss: 25.1865 - val_mean_absolute_error: 3.4365 - val_acc: 0.0000e+00\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/20\n",
      "201/201 [==============================] - 0s 307us/step - loss: 33.8942 - mean_absolute_error: 3.6849 - acc: 0.0000e+00 - val_loss: 25.1803 - val_mean_absolute_error: 3.4362 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "201/201 [==============================] - 0s 241us/step - loss: 32.7677 - mean_absolute_error: 3.7288 - acc: 0.0100 - val_loss: 25.1770 - val_mean_absolute_error: 3.4363 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "201/201 [==============================] - 0s 298us/step - loss: 30.9284 - mean_absolute_error: 3.6727 - acc: 0.0050 - val_loss: 25.1813 - val_mean_absolute_error: 3.4366 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "201/201 [==============================] - 0s 280us/step - loss: 32.5743 - mean_absolute_error: 3.6655 - acc: 0.0100 - val_loss: 25.1831 - val_mean_absolute_error: 3.4369 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "201/201 [==============================] - 0s 271us/step - loss: 33.4262 - mean_absolute_error: 3.6007 - acc: 0.0050 - val_loss: 25.1694 - val_mean_absolute_error: 3.4351 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "201/201 [==============================] - 0s 314us/step - loss: 34.4660 - mean_absolute_error: 3.7731 - acc: 0.0000e+00 - val_loss: 25.1680 - val_mean_absolute_error: 3.4351 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n",
      "201/201 [==============================] - 0s 338us/step - loss: 32.4315 - mean_absolute_error: 3.5487 - acc: 0.0000e+00 - val_loss: 25.1684 - val_mean_absolute_error: 3.4354 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "201/201 [==============================] - 0s 282us/step - loss: 33.0021 - mean_absolute_error: 3.6864 - acc: 0.0100 - val_loss: 25.1664 - val_mean_absolute_error: 3.4356 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "201/201 [==============================] - 0s 278us/step - loss: 32.5606 - mean_absolute_error: 3.6487 - acc: 0.0050 - val_loss: 25.1650 - val_mean_absolute_error: 3.4359 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "201/201 [==============================] - 0s 298us/step - loss: 32.2130 - mean_absolute_error: 3.6111 - acc: 0.0000e+00 - val_loss: 25.1636 - val_mean_absolute_error: 3.4359 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "201/201 [==============================] - 0s 299us/step - loss: 31.1706 - mean_absolute_error: 3.5647 - acc: 0.0050 - val_loss: 25.1617 - val_mean_absolute_error: 3.4358 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "201/201 [==============================] - 0s 233us/step - loss: 33.8028 - mean_absolute_error: 3.7258 - acc: 0.0050 - val_loss: 25.1601 - val_mean_absolute_error: 3.4356 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "201/201 [==============================] - 0s 263us/step - loss: 31.7075 - mean_absolute_error: 3.4504 - acc: 0.0050 - val_loss: 25.1584 - val_mean_absolute_error: 3.4360 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "201/201 [==============================] - 0s 227us/step - loss: 30.8191 - mean_absolute_error: 3.5316 - acc: 0.0050 - val_loss: 25.1573 - val_mean_absolute_error: 3.4356 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "201/201 [==============================] - 0s 257us/step - loss: 33.0675 - mean_absolute_error: 3.7550 - acc: 0.0100 - val_loss: 25.1568 - val_mean_absolute_error: 3.4352 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "201/201 [==============================] - 0s 242us/step - loss: 32.2045 - mean_absolute_error: 3.5702 - acc: 0.0149 - val_loss: 25.1534 - val_mean_absolute_error: 3.4350 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "201/201 [==============================] - 0s 251us/step - loss: 30.5726 - mean_absolute_error: 3.5487 - acc: 0.0000e+00 - val_loss: 25.1559 - val_mean_absolute_error: 3.4351 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "201/201 [==============================] - 0s 304us/step - loss: 32.3756 - mean_absolute_error: 3.5964 - acc: 0.0000e+00 - val_loss: 25.1526 - val_mean_absolute_error: 3.4357 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "201/201 [==============================] - 0s 344us/step - loss: 33.0734 - mean_absolute_error: 3.7401 - acc: 0.0050 - val_loss: 25.1520 - val_mean_absolute_error: 3.4348 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "201/201 [==============================] - 0s 440us/step - loss: 31.8016 - mean_absolute_error: 3.6624 - acc: 0.0100 - val_loss: 25.1509 - val_mean_absolute_error: 3.4350 - val_acc: 0.0000e+00\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/2\n",
      "201/201 [==============================] - 0s 381us/step - loss: 30.4490 - mean_absolute_error: 3.4902 - acc: 0.0050 - val_loss: 25.1545 - val_mean_absolute_error: 3.4353 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "201/201 [==============================] - 0s 385us/step - loss: 32.0461 - mean_absolute_error: 3.5955 - acc: 0.0000e+00 - val_loss: 25.1453 - val_mean_absolute_error: 3.4355 - val_acc: 0.0000e+00\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/2\n",
      "201/201 [==============================] - 0s 372us/step - loss: 33.0351 - mean_absolute_error: 3.5908 - acc: 0.0050 - val_loss: 25.1489 - val_mean_absolute_error: 3.4362 - val_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "201/201 [==============================] - 0s 321us/step - loss: 33.3053 - mean_absolute_error: 3.6363 - acc: 0.0000e+00 - val_loss: 25.1463 - val_mean_absolute_error: 3.4361 - val_acc: 0.0000e+00\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/1\n",
      "201/201 [==============================] - 0s 314us/step - loss: 30.9552 - mean_absolute_error: 3.5306 - acc: 0.0100 - val_loss: 25.1484 - val_mean_absolute_error: 3.4368 - val_acc: 0.0000e+00\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/5\n",
      "201/201 [==============================] - 0s 422us/step - loss: 30.8711 - mean_absolute_error: 3.4605 - acc: 0.0149 - val_loss: 25.1487 - val_mean_absolute_error: 3.4359 - val_acc: 0.0000e+00\n",
      "Epoch 2/5\n",
      "201/201 [==============================] - 0s 452us/step - loss: 30.6292 - mean_absolute_error: 3.5063 - acc: 0.0050 - val_loss: 25.1491 - val_mean_absolute_error: 3.4365 - val_acc: 0.0000e+00\n",
      "Epoch 3/5\n",
      "201/201 [==============================] - 0s 349us/step - loss: 31.5323 - mean_absolute_error: 3.4823 - acc: 0.0149 - val_loss: 25.1523 - val_mean_absolute_error: 3.4359 - val_acc: 0.0000e+00\n",
      "Epoch 4/5\n",
      "201/201 [==============================] - 0s 407us/step - loss: 30.6579 - mean_absolute_error: 3.5125 - acc: 0.0050 - val_loss: 25.1495 - val_mean_absolute_error: 3.4360 - val_acc: 0.0000e+00\n",
      "Epoch 5/5\n",
      "201/201 [==============================] - 0s 205us/step - loss: 29.8689 - mean_absolute_error: 3.4379 - acc: 0.0100 - val_loss: 25.1466 - val_mean_absolute_error: 3.4366 - val_acc: 0.0000e+00\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/20\n",
      "201/201 [==============================] - 0s 336us/step - loss: 30.7532 - mean_absolute_error: 3.4497 - acc: 0.0100 - val_loss: 25.1525 - val_mean_absolute_error: 3.4354 - val_acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "201/201 [==============================] - 0s 236us/step - loss: 30.8339 - mean_absolute_error: 3.5762 - acc: 0.0050 - val_loss: 25.1471 - val_mean_absolute_error: 3.4352 - val_acc: 0.0000e+00\n",
      "Epoch 3/20\n",
      "201/201 [==============================] - 0s 281us/step - loss: 30.9828 - mean_absolute_error: 3.4819 - acc: 0.0000e+00 - val_loss: 25.1411 - val_mean_absolute_error: 3.4341 - val_acc: 0.0000e+00\n",
      "Epoch 4/20\n",
      "201/201 [==============================] - 0s 324us/step - loss: 30.3327 - mean_absolute_error: 3.3984 - acc: 0.0050 - val_loss: 25.1356 - val_mean_absolute_error: 3.4344 - val_acc: 0.0000e+00\n",
      "Epoch 5/20\n",
      "201/201 [==============================] - 0s 336us/step - loss: 29.6787 - mean_absolute_error: 3.3995 - acc: 0.0050 - val_loss: 25.1373 - val_mean_absolute_error: 3.4338 - val_acc: 0.0000e+00\n",
      "Epoch 6/20\n",
      "201/201 [==============================] - 0s 393us/step - loss: 32.1464 - mean_absolute_error: 3.5267 - acc: 0.0100 - val_loss: 25.1309 - val_mean_absolute_error: 3.4340 - val_acc: 0.0000e+00\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201/201 [==============================] - 0s 393us/step - loss: 29.6826 - mean_absolute_error: 3.4166 - acc: 0.0100 - val_loss: 25.1351 - val_mean_absolute_error: 3.4341 - val_acc: 0.0000e+00\n",
      "Epoch 8/20\n",
      "201/201 [==============================] - 0s 398us/step - loss: 30.1727 - mean_absolute_error: 3.4805 - acc: 0.0050 - val_loss: 25.1480 - val_mean_absolute_error: 3.4351 - val_acc: 0.0000e+00\n",
      "Epoch 9/20\n",
      "201/201 [==============================] - 0s 393us/step - loss: 30.2753 - mean_absolute_error: 3.4611 - acc: 0.0050 - val_loss: 25.1482 - val_mean_absolute_error: 3.4361 - val_acc: 0.0000e+00\n",
      "Epoch 10/20\n",
      "201/201 [==============================] - 0s 304us/step - loss: 27.7521 - mean_absolute_error: 3.4075 - acc: 0.0050 - val_loss: 25.1409 - val_mean_absolute_error: 3.4344 - val_acc: 0.0000e+00\n",
      "Epoch 11/20\n",
      "201/201 [==============================] - 0s 269us/step - loss: 29.1629 - mean_absolute_error: 3.4434 - acc: 0.0100 - val_loss: 25.1401 - val_mean_absolute_error: 3.4343 - val_acc: 0.0000e+00\n",
      "Epoch 12/20\n",
      "201/201 [==============================] - 0s 316us/step - loss: 29.5813 - mean_absolute_error: 3.4774 - acc: 0.0050 - val_loss: 25.1439 - val_mean_absolute_error: 3.4347 - val_acc: 0.0000e+00\n",
      "Epoch 13/20\n",
      "201/201 [==============================] - 0s 291us/step - loss: 28.2594 - mean_absolute_error: 3.3246 - acc: 0.0050 - val_loss: 25.1422 - val_mean_absolute_error: 3.4332 - val_acc: 0.0000e+00\n",
      "Epoch 14/20\n",
      "201/201 [==============================] - 0s 308us/step - loss: 28.2276 - mean_absolute_error: 3.3275 - acc: 0.0050 - val_loss: 25.1338 - val_mean_absolute_error: 3.4332 - val_acc: 0.0000e+00\n",
      "Epoch 15/20\n",
      "201/201 [==============================] - 0s 345us/step - loss: 30.0316 - mean_absolute_error: 3.4454 - acc: 0.0050 - val_loss: 25.1456 - val_mean_absolute_error: 3.4341 - val_acc: 0.0000e+00\n",
      "Epoch 16/20\n",
      "201/201 [==============================] - 0s 254us/step - loss: 28.2828 - mean_absolute_error: 3.3658 - acc: 0.0050 - val_loss: 25.1531 - val_mean_absolute_error: 3.4351 - val_acc: 0.0000e+00\n",
      "Epoch 17/20\n",
      "201/201 [==============================] - 0s 303us/step - loss: 28.8459 - mean_absolute_error: 3.4160 - acc: 0.0050 - val_loss: 25.1434 - val_mean_absolute_error: 3.4360 - val_acc: 0.0000e+00\n",
      "Epoch 18/20\n",
      "201/201 [==============================] - 0s 320us/step - loss: 28.9319 - mean_absolute_error: 3.3735 - acc: 0.0100 - val_loss: 25.1430 - val_mean_absolute_error: 3.4376 - val_acc: 0.0000e+00\n",
      "Epoch 19/20\n",
      "201/201 [==============================] - 0s 267us/step - loss: 28.9960 - mean_absolute_error: 3.3162 - acc: 0.0050 - val_loss: 25.1607 - val_mean_absolute_error: 3.4372 - val_acc: 0.0000e+00\n",
      "Epoch 20/20\n",
      "201/201 [==============================] - 0s 297us/step - loss: 30.2645 - mean_absolute_error: 3.4587 - acc: 0.0000e+00 - val_loss: 25.1745 - val_mean_absolute_error: 3.4365 - val_acc: 0.0000e+00\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/40\n",
      "201/201 [==============================] - 0s 299us/step - loss: 29.3011 - mean_absolute_error: 3.4800 - acc: 0.0000e+00 - val_loss: 25.1767 - val_mean_absolute_error: 3.4367 - val_acc: 0.0000e+00\n",
      "Epoch 2/40\n",
      "201/201 [==============================] - 0s 284us/step - loss: 28.6792 - mean_absolute_error: 3.3443 - acc: 0.0050 - val_loss: 25.1705 - val_mean_absolute_error: 3.4376 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "201/201 [==============================] - 0s 290us/step - loss: 29.3932 - mean_absolute_error: 3.4867 - acc: 0.0050 - val_loss: 25.1926 - val_mean_absolute_error: 3.4386 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "201/201 [==============================] - 0s 287us/step - loss: 30.3378 - mean_absolute_error: 3.4732 - acc: 0.0100 - val_loss: 25.2056 - val_mean_absolute_error: 3.4393 - val_acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      "201/201 [==============================] - 0s 303us/step - loss: 30.2100 - mean_absolute_error: 3.5135 - acc: 0.0000e+00 - val_loss: 25.1990 - val_mean_absolute_error: 3.4417 - val_acc: 0.0000e+00\n",
      "Epoch 6/40\n",
      "201/201 [==============================] - 0s 328us/step - loss: 28.9115 - mean_absolute_error: 3.4607 - acc: 0.0100 - val_loss: 25.2063 - val_mean_absolute_error: 3.4445 - val_acc: 0.0000e+00\n",
      "Epoch 7/40\n",
      "201/201 [==============================] - 0s 287us/step - loss: 29.1158 - mean_absolute_error: 3.4089 - acc: 0.0050 - val_loss: 25.2068 - val_mean_absolute_error: 3.4441 - val_acc: 0.0000e+00\n",
      "Epoch 8/40\n",
      "201/201 [==============================] - 0s 257us/step - loss: 29.5998 - mean_absolute_error: 3.4065 - acc: 0.0050 - val_loss: 25.1971 - val_mean_absolute_error: 3.4446 - val_acc: 0.0000e+00\n",
      "Epoch 9/40\n",
      "201/201 [==============================] - 0s 312us/step - loss: 27.0920 - mean_absolute_error: 3.2125 - acc: 0.0100 - val_loss: 25.1728 - val_mean_absolute_error: 3.4449 - val_acc: 0.0000e+00\n",
      "Epoch 10/40\n",
      "201/201 [==============================] - 0s 261us/step - loss: 28.7474 - mean_absolute_error: 3.3922 - acc: 0.0050 - val_loss: 25.1995 - val_mean_absolute_error: 3.4428 - val_acc: 0.0000e+00\n",
      "Epoch 11/40\n",
      "201/201 [==============================] - 0s 254us/step - loss: 27.3457 - mean_absolute_error: 3.2540 - acc: 0.0100 - val_loss: 25.1906 - val_mean_absolute_error: 3.4431 - val_acc: 0.0000e+00\n",
      "Epoch 12/40\n",
      "201/201 [==============================] - 0s 216us/step - loss: 28.6824 - mean_absolute_error: 3.3538 - acc: 0.0050 - val_loss: 25.1899 - val_mean_absolute_error: 3.4431 - val_acc: 0.0000e+00\n",
      "Epoch 13/40\n",
      "201/201 [==============================] - 0s 262us/step - loss: 27.6091 - mean_absolute_error: 3.3021 - acc: 0.0050 - val_loss: 25.1919 - val_mean_absolute_error: 3.4438 - val_acc: 0.0000e+00\n",
      "Epoch 14/40\n",
      "201/201 [==============================] - 0s 308us/step - loss: 28.3660 - mean_absolute_error: 3.2933 - acc: 0.0100 - val_loss: 25.1771 - val_mean_absolute_error: 3.4423 - val_acc: 0.0000e+00\n",
      "Epoch 15/40\n",
      "201/201 [==============================] - 0s 242us/step - loss: 26.2310 - mean_absolute_error: 3.1251 - acc: 0.0000e+00 - val_loss: 25.2119 - val_mean_absolute_error: 3.4447 - val_acc: 0.0000e+00\n",
      "Epoch 16/40\n",
      "201/201 [==============================] - 0s 262us/step - loss: 27.7977 - mean_absolute_error: 3.3176 - acc: 0.0000e+00 - val_loss: 25.2123 - val_mean_absolute_error: 3.4450 - val_acc: 0.0000e+00\n",
      "Epoch 17/40\n",
      "201/201 [==============================] - 0s 273us/step - loss: 27.8572 - mean_absolute_error: 3.3618 - acc: 0.0100 - val_loss: 25.2192 - val_mean_absolute_error: 3.4452 - val_acc: 0.0000e+00\n",
      "Epoch 18/40\n",
      "201/201 [==============================] - 0s 295us/step - loss: 28.2261 - mean_absolute_error: 3.3916 - acc: 0.0100 - val_loss: 25.2275 - val_mean_absolute_error: 3.4450 - val_acc: 0.0000e+00\n",
      "Epoch 19/40\n",
      "201/201 [==============================] - 0s 267us/step - loss: 26.6284 - mean_absolute_error: 3.2298 - acc: 0.0100 - val_loss: 25.2213 - val_mean_absolute_error: 3.4453 - val_acc: 0.0000e+00\n",
      "Epoch 20/40\n",
      "201/201 [==============================] - 0s 277us/step - loss: 28.0065 - mean_absolute_error: 3.3626 - acc: 0.0100 - val_loss: 25.2180 - val_mean_absolute_error: 3.4471 - val_acc: 0.0000e+00\n",
      "Epoch 21/40\n",
      "201/201 [==============================] - 0s 241us/step - loss: 26.9523 - mean_absolute_error: 3.2933 - acc: 0.0050 - val_loss: 25.2258 - val_mean_absolute_error: 3.4453 - val_acc: 0.0000e+00\n",
      "Epoch 22/40\n",
      "201/201 [==============================] - 0s 263us/step - loss: 26.9728 - mean_absolute_error: 3.2063 - acc: 0.0050 - val_loss: 25.2307 - val_mean_absolute_error: 3.4462 - val_acc: 0.0000e+00\n",
      "Epoch 23/40\n",
      "201/201 [==============================] - 0s 285us/step - loss: 28.2552 - mean_absolute_error: 3.3436 - acc: 0.0100 - val_loss: 25.2361 - val_mean_absolute_error: 3.4484 - val_acc: 0.0000e+00\n",
      "Epoch 24/40\n",
      "201/201 [==============================] - 0s 235us/step - loss: 29.6880 - mean_absolute_error: 3.4703 - acc: 0.0100 - val_loss: 25.2262 - val_mean_absolute_error: 3.4481 - val_acc: 0.0000e+00\n",
      "Epoch 25/40\n",
      "201/201 [==============================] - 0s 288us/step - loss: 26.9695 - mean_absolute_error: 3.2653 - acc: 0.0100 - val_loss: 25.2429 - val_mean_absolute_error: 3.4478 - val_acc: 0.0000e+00\n",
      "Epoch 26/40\n",
      "201/201 [==============================] - 0s 297us/step - loss: 26.4357 - mean_absolute_error: 3.2050 - acc: 0.0000e+00 - val_loss: 25.2617 - val_mean_absolute_error: 3.4482 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40\n",
      "201/201 [==============================] - 0s 318us/step - loss: 26.7005 - mean_absolute_error: 3.2750 - acc: 0.0000e+00 - val_loss: 25.2569 - val_mean_absolute_error: 3.4500 - val_acc: 0.0000e+00\n",
      "Epoch 28/40\n",
      "201/201 [==============================] - 0s 290us/step - loss: 25.5387 - mean_absolute_error: 3.1280 - acc: 0.0100 - val_loss: 25.2314 - val_mean_absolute_error: 3.4489 - val_acc: 0.0000e+00\n",
      "Epoch 29/40\n",
      "201/201 [==============================] - 0s 282us/step - loss: 26.8583 - mean_absolute_error: 3.1829 - acc: 0.0050 - val_loss: 25.2434 - val_mean_absolute_error: 3.4535 - val_acc: 0.0000e+00\n",
      "Epoch 30/40\n",
      "201/201 [==============================] - 0s 276us/step - loss: 26.5481 - mean_absolute_error: 3.2137 - acc: 0.0000e+00 - val_loss: 25.2285 - val_mean_absolute_error: 3.4522 - val_acc: 0.0000e+00\n",
      "Epoch 31/40\n",
      "201/201 [==============================] - 0s 293us/step - loss: 25.9202 - mean_absolute_error: 3.1561 - acc: 0.0050 - val_loss: 25.2245 - val_mean_absolute_error: 3.4546 - val_acc: 0.0000e+00\n",
      "Epoch 32/40\n",
      "201/201 [==============================] - 0s 241us/step - loss: 26.6456 - mean_absolute_error: 3.1725 - acc: 0.0000e+00 - val_loss: 25.2078 - val_mean_absolute_error: 3.4572 - val_acc: 0.0000e+00\n",
      "Epoch 33/40\n",
      "201/201 [==============================] - 0s 296us/step - loss: 25.1951 - mean_absolute_error: 3.1671 - acc: 0.0050 - val_loss: 25.2203 - val_mean_absolute_error: 3.4555 - val_acc: 0.0000e+00\n",
      "Epoch 34/40\n",
      "201/201 [==============================] - 0s 269us/step - loss: 26.4237 - mean_absolute_error: 3.1509 - acc: 0.0000e+00 - val_loss: 25.2298 - val_mean_absolute_error: 3.4560 - val_acc: 0.0000e+00\n",
      "Epoch 35/40\n",
      "201/201 [==============================] - 0s 237us/step - loss: 27.4352 - mean_absolute_error: 3.3049 - acc: 0.0000e+00 - val_loss: 25.2266 - val_mean_absolute_error: 3.4604 - val_acc: 0.0000e+00\n",
      "Epoch 36/40\n",
      "201/201 [==============================] - 0s 296us/step - loss: 27.7523 - mean_absolute_error: 3.3355 - acc: 0.0000e+00 - val_loss: 25.1990 - val_mean_absolute_error: 3.4596 - val_acc: 0.0000e+00\n",
      "Epoch 37/40\n",
      "201/201 [==============================] - 0s 300us/step - loss: 26.7457 - mean_absolute_error: 3.2120 - acc: 0.0000e+00 - val_loss: 25.1937 - val_mean_absolute_error: 3.4589 - val_acc: 0.0000e+00\n",
      "Epoch 38/40\n",
      "201/201 [==============================] - 0s 271us/step - loss: 26.0659 - mean_absolute_error: 3.2116 - acc: 0.0050 - val_loss: 25.1966 - val_mean_absolute_error: 3.4566 - val_acc: 0.0000e+00\n",
      "Epoch 39/40\n",
      "201/201 [==============================] - 0s 269us/step - loss: 26.6103 - mean_absolute_error: 3.2475 - acc: 0.0149 - val_loss: 25.2417 - val_mean_absolute_error: 3.4554 - val_acc: 0.0000e+00\n",
      "Epoch 40/40\n",
      "201/201 [==============================] - 0s 322us/step - loss: 26.7779 - mean_absolute_error: 3.1617 - acc: 0.0050 - val_loss: 25.2306 - val_mean_absolute_error: 3.4550 - val_acc: 0.0000e+00\n",
      "Train on 201 samples, validate on 51 samples\n",
      "Epoch 1/80\n",
      "201/201 [==============================] - 0s 268us/step - loss: 27.3392 - mean_absolute_error: 3.1999 - acc: 0.0000e+00 - val_loss: 25.2276 - val_mean_absolute_error: 3.4615 - val_acc: 0.0000e+00\n",
      "Epoch 2/80\n",
      "201/201 [==============================] - 0s 256us/step - loss: 26.6737 - mean_absolute_error: 3.1796 - acc: 0.0000e+00 - val_loss: 25.2177 - val_mean_absolute_error: 3.4608 - val_acc: 0.0000e+00\n",
      "Epoch 3/80\n",
      "201/201 [==============================] - 0s 255us/step - loss: 26.4591 - mean_absolute_error: 3.1629 - acc: 0.0050 - val_loss: 25.2207 - val_mean_absolute_error: 3.4643 - val_acc: 0.0000e+00\n",
      "Epoch 4/80\n",
      "201/201 [==============================] - 0s 342us/step - loss: 25.7672 - mean_absolute_error: 3.1844 - acc: 0.0050 - val_loss: 25.2172 - val_mean_absolute_error: 3.4617 - val_acc: 0.0000e+00\n",
      "Epoch 5/80\n",
      "201/201 [==============================] - 0s 322us/step - loss: 24.5254 - mean_absolute_error: 3.0353 - acc: 0.0050 - val_loss: 25.2604 - val_mean_absolute_error: 3.4567 - val_acc: 0.0000e+00\n",
      "Epoch 6/80\n",
      "201/201 [==============================] - 0s 307us/step - loss: 25.4784 - mean_absolute_error: 3.1461 - acc: 0.0100 - val_loss: 25.2506 - val_mean_absolute_error: 3.4572 - val_acc: 0.0000e+00\n",
      "Epoch 7/80\n",
      "201/201 [==============================] - 0s 333us/step - loss: 25.3690 - mean_absolute_error: 3.2225 - acc: 0.0050 - val_loss: 25.2707 - val_mean_absolute_error: 3.4622 - val_acc: 0.0000e+00\n",
      "Epoch 8/80\n",
      "201/201 [==============================] - 0s 320us/step - loss: 26.5130 - mean_absolute_error: 3.1527 - acc: 0.0149 - val_loss: 25.2170 - val_mean_absolute_error: 3.4596 - val_acc: 0.0000e+00\n",
      "Epoch 9/80\n",
      "201/201 [==============================] - 0s 320us/step - loss: 26.1892 - mean_absolute_error: 3.1371 - acc: 0.0050 - val_loss: 25.1872 - val_mean_absolute_error: 3.4590 - val_acc: 0.0000e+00\n",
      "Epoch 10/80\n",
      "201/201 [==============================] - 0s 278us/step - loss: 26.1054 - mean_absolute_error: 3.1865 - acc: 0.0000e+00 - val_loss: 25.1616 - val_mean_absolute_error: 3.4606 - val_acc: 0.0000e+00\n",
      "Epoch 11/80\n",
      "201/201 [==============================] - 0s 314us/step - loss: 24.6608 - mean_absolute_error: 2.9845 - acc: 0.0100 - val_loss: 25.1577 - val_mean_absolute_error: 3.4584 - val_acc: 0.0000e+00\n",
      "Epoch 12/80\n",
      "201/201 [==============================] - 0s 233us/step - loss: 26.6245 - mean_absolute_error: 3.1989 - acc: 0.0050 - val_loss: 25.1417 - val_mean_absolute_error: 3.4638 - val_acc: 0.0000e+00\n",
      "Epoch 13/80\n",
      "201/201 [==============================] - 0s 254us/step - loss: 27.7648 - mean_absolute_error: 3.2808 - acc: 0.0100 - val_loss: 25.0751 - val_mean_absolute_error: 3.4632 - val_acc: 0.0000e+00\n",
      "Epoch 14/80\n",
      "201/201 [==============================] - 0s 285us/step - loss: 25.4666 - mean_absolute_error: 3.1478 - acc: 0.0050 - val_loss: 25.0720 - val_mean_absolute_error: 3.4687 - val_acc: 0.0000e+00\n",
      "Epoch 15/80\n",
      "201/201 [==============================] - 0s 266us/step - loss: 26.6423 - mean_absolute_error: 3.2502 - acc: 0.0000e+00 - val_loss: 25.0828 - val_mean_absolute_error: 3.4681 - val_acc: 0.0000e+00\n",
      "Epoch 16/80\n",
      "201/201 [==============================] - 0s 256us/step - loss: 25.7995 - mean_absolute_error: 3.1565 - acc: 0.0050 - val_loss: 25.1156 - val_mean_absolute_error: 3.4684 - val_acc: 0.0000e+00\n",
      "Epoch 17/80\n",
      "201/201 [==============================] - 0s 218us/step - loss: 24.7115 - mean_absolute_error: 3.1231 - acc: 0.0000e+00 - val_loss: 25.1318 - val_mean_absolute_error: 3.4655 - val_acc: 0.0000e+00\n",
      "Epoch 18/80\n",
      "201/201 [==============================] - 0s 221us/step - loss: 25.4423 - mean_absolute_error: 3.0932 - acc: 0.0050 - val_loss: 25.1588 - val_mean_absolute_error: 3.4625 - val_acc: 0.0000e+00\n",
      "Epoch 19/80\n",
      "201/201 [==============================] - 0s 320us/step - loss: 25.2203 - mean_absolute_error: 3.0790 - acc: 0.0000e+00 - val_loss: 25.1604 - val_mean_absolute_error: 3.4639 - val_acc: 0.0000e+00\n",
      "Epoch 20/80\n",
      "201/201 [==============================] - 0s 267us/step - loss: 24.0941 - mean_absolute_error: 3.0194 - acc: 0.0000e+00 - val_loss: 25.1560 - val_mean_absolute_error: 3.4618 - val_acc: 0.0000e+00\n",
      "Epoch 21/80\n",
      "201/201 [==============================] - 0s 289us/step - loss: 25.1857 - mean_absolute_error: 3.0489 - acc: 0.0100 - val_loss: 25.1515 - val_mean_absolute_error: 3.4579 - val_acc: 0.0000e+00\n",
      "Epoch 22/80\n",
      "201/201 [==============================] - 0s 321us/step - loss: 24.2189 - mean_absolute_error: 3.1096 - acc: 0.0050 - val_loss: 25.1437 - val_mean_absolute_error: 3.4570 - val_acc: 0.0000e+00\n",
      "Epoch 23/80\n",
      " 64/201 [========>.....................] - ETA: 0s - loss: 15.4105 - mean_absolute_error: 2.8301 - acc: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "for info in sched:\n",
    "    lr, epochs = info\n",
    "    m1.optimizer.lr = lr\n",
    "    m1.fit(np.array(X), np.array(y)*100, epochs=epochs,  batch_size=64, validation_data=(np.array(x_val), np.array(y_val)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00646421]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03393828]]\n",
      "-0.0005292405398252453\n",
      "\n",
      "\n",
      "[[-0.03410605]]\n",
      "0.008333333333333255\n",
      "\n",
      "\n",
      "[[-0.01393568]]\n",
      "-0.04828973843058361\n",
      "\n",
      "\n",
      "[[-0.05682974]]\n",
      "0.008258680999914739\n",
      "\n",
      "\n",
      "[[-0.06315508]]\n",
      "-0.03969128996692402\n",
      "\n",
      "\n",
      "[[ 0.0414346]]\n",
      "-0.010504201680672232\n",
      "\n",
      "\n",
      "[[-0.07775185]]\n",
      "0.00330429558425952\n",
      "\n",
      "\n",
      "[[-0.01503236]]\n",
      "-0.017325680272108797\n",
      "\n",
      "\n",
      "[[-0.05437827]]\n",
      "0.0689941812136326\n",
      "\n",
      "\n",
      "[[-0.01032812]]\n",
      "-0.004714757190004782\n",
      "\n",
      "\n",
      "[[ 0.03748613]]\n",
      "0.11038961038961026\n",
      "\n",
      "\n",
      "[[ 0.03277944]]\n",
      "0.06974477958236659\n",
      "\n",
      "\n",
      "[[ 0.0618001]]\n",
      "0.060831509846826996\n",
      "\n",
      "\n",
      "[[-0.00046651]]\n",
      "-0.018439716312056723\n",
      "\n",
      "\n",
      "[[-0.0925583]]\n",
      "-0.019653179190751477\n",
      "\n",
      "\n",
      "[[-0.02209948]]\n",
      "0.017944535073409516\n",
      "\n",
      "\n",
      "[[ 0.01048759]]\n",
      "0.009907755380936084\n",
      "\n",
      "\n",
      "[[ 0.03909699]]\n",
      "0.03676012461059189\n",
      "\n",
      "\n",
      "[[-0.05168632]]\n",
      "0.02627257799671595\n",
      "\n",
      "\n",
      "[[-0.00342466]]\n",
      "-0.02448210922787202\n",
      "\n",
      "\n",
      "[[ 0.05398517]]\n",
      "0.05106382978723409\n",
      "\n",
      "\n",
      "[[ 0.00389128]]\n",
      "-0.021212121212121165\n",
      "\n",
      "\n",
      "[[ 0.01657477]]\n",
      "0.3072440944881889\n",
      "\n",
      "\n",
      "[[ 0.0322951]]\n",
      "0.040425531914893585\n",
      "\n",
      "\n",
      "[[-0.06094308]]\n",
      "-0.06740389678778293\n",
      "\n",
      "\n",
      "[[ 0.00656767]]\n",
      "0.013888888888888945\n",
      "\n",
      "\n",
      "[[ 0.05227538]]\n",
      "0.003148897885740004\n",
      "\n",
      "\n",
      "[[-0.05362844]]\n",
      "-0.025000000000000022\n",
      "\n",
      "\n",
      "[[ 0.0282386]]\n",
      "0.02008032128514049\n",
      "\n",
      "\n",
      "[[ 0.01268629]]\n",
      "0.021189591078066925\n",
      "\n",
      "\n",
      "[[ 0.01853379]]\n",
      "0.0010951403148528173\n",
      "\n",
      "\n",
      "[[ 0.0211322]]\n",
      "-0.015488594762038782\n",
      "\n",
      "\n",
      "[[ 0.0625845]]\n",
      "-0.04334180242116263\n",
      "\n",
      "\n",
      "[[ 0.02883643]]\n",
      "0.002918454935622347\n",
      "\n",
      "\n",
      "[[ 0.02767561]]\n",
      "-0.020833333333333353\n",
      "\n",
      "\n",
      "[[ 0.03278796]]\n",
      "0.006435643564356445\n",
      "\n",
      "\n",
      "[[ 0.01631412]]\n",
      "-0.02661169415292357\n",
      "\n",
      "\n",
      "[[-0.02126524]]\n",
      "0.042614145031333844\n",
      "\n",
      "\n",
      "[[-0.06413291]]\n",
      "-0.02059086839749326\n",
      "\n",
      "\n",
      "[[ 0.01082237]]\n",
      "-0.021964956195244176\n",
      "\n",
      "\n",
      "[[ 0.00642853]]\n",
      "-0.07228289131565266\n",
      "\n",
      "\n",
      "[[ 0.00610836]]\n",
      "0.017777777777777715\n",
      "\n",
      "\n",
      "[[-0.01769889]]\n",
      "0.05929919137466292\n",
      "\n",
      "\n",
      "[[-0.04929261]]\n",
      "-0.00055299539170509\n",
      "\n",
      "\n",
      "[[-0.05133209]]\n",
      "0.06746323529411768\n",
      "\n",
      "\n",
      "[[ 0.03339674]]\n",
      "-0.019032513877874722\n",
      "\n",
      "\n",
      "[[-0.04444509]]\n",
      "0.005444250871080139\n",
      "\n",
      "\n",
      "[[-0.04636379]]\n",
      "-0.0037672666387610115\n",
      "\n",
      "\n",
      "[[-0.01139527]]\n",
      "0.03249554264680512\n",
      "\n",
      "\n",
      "[[-0.01389003]]\n",
      "-0.007181328545780892\n",
      "\n",
      "\n",
      "[[-0.05694752]]\n",
      "0.0012210012210011516\n",
      "\n",
      "\n",
      "[[ 0.12356302]]\n",
      "0.0048992161254200525\n",
      "\n",
      "\n",
      "[[ 0.01372336]]\n",
      "0.007092198581560258\n",
      "\n",
      "\n",
      "[[ 0.00708989]]\n",
      "0.016067653276955713\n",
      "\n",
      "\n",
      "[[-0.00990351]]\n",
      "0.01775956284153004\n",
      "\n",
      "\n",
      "[[ 0.0131652]]\n",
      "-0.03681885125184094\n",
      "\n",
      "\n",
      "[[ 0.00024711]]\n",
      "-0.025644871025794956\n",
      "\n",
      "\n",
      "[[ 0.0245587]]\n",
      "0.021448999046711117\n",
      "\n",
      "\n",
      "[[-0.02963039]]\n",
      "-0.01939557961208833\n",
      "\n",
      "\n",
      "[[-0.00704311]]\n",
      "-0.0745341614906833\n",
      "\n",
      "\n",
      "[[ 0.02483589]]\n",
      "-0.0218858289257707\n",
      "\n",
      "\n",
      "[[ 0.06413695]]\n",
      "0.03477800335422365\n",
      "\n",
      "\n",
      "[[-0.02590221]]\n",
      "0.022781954887218053\n",
      "\n",
      "\n",
      "[[-0.04061615]]\n",
      "-0.0308597674752405\n",
      "\n",
      "\n",
      "[[ 0.02918044]]\n",
      "0.015151515151515072\n",
      "\n",
      "\n",
      "[[-0.00274047]]\n",
      "0.01530091805508328\n",
      "\n",
      "\n",
      "[[ 0.14134645]]\n",
      "0.014453781512605112\n",
      "\n",
      "\n",
      "[[-0.03665792]]\n",
      "0.025385116077240064\n",
      "\n",
      "\n",
      "[[-0.07229802]]\n",
      "-0.016725128361547558\n",
      "\n",
      "\n",
      "[[-0.05784817]]\n",
      "0.007243460764587513\n",
      "\n",
      "\n",
      "[[ 0.00784561]]\n",
      "-0.07304643261608142\n",
      "\n",
      "\n",
      "[[-0.0131195]]\n",
      "0.024353876739562547\n",
      "\n",
      "\n",
      "[[ 0.0108724]]\n",
      "-0.04166666666666679\n",
      "\n",
      "\n",
      "[[-0.10192973]]\n",
      "-0.00234820530023483\n",
      "\n",
      "\n",
      "[[-0.02882341]]\n",
      "-0.036093418259023416\n",
      "\n",
      "\n",
      "[[-0.03271881]]\n",
      "-0.03178484107579455\n",
      "\n",
      "\n",
      "[[-0.0239385]]\n",
      "-0.0018992493443067175\n",
      "\n",
      "\n",
      "[[ 0.01687495]]\n",
      "-0.07429588863709943\n",
      "\n",
      "\n",
      "[[-0.06922627]]\n",
      "-0.01622971285892631\n",
      "\n",
      "\n",
      "[[ 0.02547575]]\n",
      "0.14176245210727975\n",
      "\n",
      "\n",
      "[[-0.00168196]]\n",
      "0.014298480786416457\n",
      "\n",
      "\n",
      "[[-0.00320476]]\n",
      "-0.01745635910224435\n",
      "\n",
      "\n",
      "[[-0.06509983]]\n",
      "-0.02686308492201047\n",
      "\n",
      "\n",
      "[[ 0.10623275]]\n",
      "0.10058027079303684\n",
      "\n",
      "\n",
      "[[ 0.02996561]]\n",
      "-0.014939309056956063\n",
      "\n",
      "\n",
      "[[ 0.00598266]]\n",
      "0.038142189932537596\n",
      "\n",
      "\n",
      "[[ 0.02448836]]\n",
      "0.07006369426751602\n",
      "\n",
      "\n",
      "[[-0.02146034]]\n",
      "-0.024442538593482003\n",
      "\n",
      "\n",
      "[[-0.06782688]]\n",
      "-0.007194244604316521\n",
      "\n",
      "\n",
      "[[ 0.00024814]]\n",
      "-0.06844876468205743\n",
      "\n",
      "\n",
      "[[-0.12493977]]\n",
      "-0.02002556455048994\n",
      "\n",
      "\n",
      "[[ 0.0446946]]\n",
      "0.029530978575564585\n",
      "\n",
      "\n",
      "[[-0.10868913]]\n",
      "-0.03367570298029971\n",
      "\n",
      "\n",
      "[[-0.00842824]]\n",
      "0.0483641536273115\n",
      "\n",
      "\n",
      "[[ 0.03065239]]\n",
      "-0.07028824833702886\n",
      "\n",
      "\n",
      "[[ 0.05995681]]\n",
      "-0.06535236510273303\n",
      "\n",
      "\n",
      "[[ 0.00949987]]\n",
      "0.018121693121693185\n",
      "\n",
      "\n",
      "[[-0.06310956]]\n",
      "-0.05465346534653461\n",
      "\n",
      "\n",
      "[[-0.00392679]]\n",
      "0.09901202749140892\n",
      "\n",
      "\n",
      "[[ 0.05517564]]\n",
      "0.03010890454836636\n",
      "\n",
      "\n",
      "[[-0.02531474]]\n",
      "-0.016586040082930215\n",
      "\n",
      "\n",
      "[[ 0.0150385]]\n",
      "-0.014742014742014678\n",
      "\n",
      "\n",
      "[[ 0.06417951]]\n",
      "0.0\n",
      "\n",
      "\n",
      "[[-0.01836102]]\n",
      "-0.06380368098159514\n",
      "\n",
      "\n",
      "[[ 0.00013386]]\n",
      "0.041666666666666664\n",
      "\n",
      "\n",
      "[[-0.01743512]]\n",
      "-0.07023411371237472\n",
      "\n",
      "\n",
      "[[ 0.09717651]]\n",
      "0.02774193548387095\n",
      "\n",
      "\n",
      "[[-0.06878468]]\n",
      "0.017316017316017254\n",
      "\n",
      "\n",
      "[[ 0.05797933]]\n",
      "-0.015544041450777177\n",
      "\n",
      "\n",
      "[[-0.00708905]]\n",
      "-0.012087026591458501\n",
      "\n",
      "\n",
      "[[ 0.1026746]]\n",
      "0.02243229432805322\n",
      "\n",
      "\n",
      "[[-0.05700441]]\n",
      "-0.10233050847457638\n",
      "\n",
      "\n",
      "[[-0.0072142]]\n",
      "-0.011323701679031595\n",
      "\n",
      "\n",
      "[[ 0.02333731]]\n",
      "-0.016192345436702567\n",
      "\n",
      "\n",
      "[[-0.06562023]]\n",
      "-0.08683666436940041\n",
      "\n",
      "\n",
      "[[ 0.04412393]]\n",
      "-0.046251398731816606\n",
      "\n",
      "\n",
      "[[-0.01661597]]\n",
      "0.01110005358646572\n",
      "\n",
      "\n",
      "[[ 0.03973384]]\n",
      "0.05226480836236934\n",
      "\n",
      "\n",
      "[[ 0.01866118]]\n",
      "0.0010235414534289027\n",
      "\n",
      "\n",
      "[[ 0.00545077]]\n",
      "-0.008037860576923026\n",
      "\n",
      "\n",
      "[[-0.01829196]]\n",
      "0.0\n",
      "\n",
      "\n",
      "[[-0.07810028]]\n",
      "-0.019500438212094667\n",
      "\n",
      "\n",
      "[[-0.01008063]]\n",
      "0.10714285714285725\n",
      "\n",
      "\n",
      "[[-0.04461472]]\n",
      "-0.008746355685131267\n",
      "\n",
      "\n",
      "[[ 0.0297654]]\n",
      "0.08130081300813008\n",
      "\n",
      "\n",
      "[[-0.03144908]]\n",
      "-0.012875536480686725\n",
      "\n",
      "\n",
      "[[-0.00482863]]\n",
      "-0.0030581039755352116\n",
      "\n",
      "\n",
      "[[-0.05844922]]\n",
      "-0.03787103377686789\n",
      "\n",
      "\n",
      "[[ 0.03900871]]\n",
      "-0.008660624370594153\n",
      "\n",
      "\n",
      "[[-0.05378702]]\n",
      "0.004132231404958737\n",
      "\n",
      "\n",
      "[[-0.00551547]]\n",
      "-0.027699968876439482\n",
      "\n",
      "\n",
      "[[ 0.00902168]]\n",
      "0.01167883211678828\n",
      "\n",
      "\n",
      "[[-0.04136119]]\n",
      "-0.09026845637583897\n",
      "\n",
      "\n",
      "[[ 0.03463724]]\n",
      "-0.0684817103724736\n",
      "\n",
      "\n",
      "[[ 0.04566149]]\n",
      "-0.0140734949179047\n",
      "\n",
      "\n",
      "[[-0.05754401]]\n",
      "0.01910531220876049\n",
      "\n",
      "\n",
      "[[ 0.05724907]]\n",
      "-0.007294429708222774\n",
      "\n",
      "\n",
      "[[ 0.01765279]]\n",
      "-0.01249353782526286\n",
      "\n",
      "\n",
      "[[ 0.03227338]]\n",
      "0.14824955994523759\n",
      "\n",
      "\n",
      "[[-0.03381896]]\n",
      "-0.005627789637104556\n",
      "\n",
      "\n",
      "[[-0.05260278]]\n",
      "0.3177777777777777\n",
      "\n",
      "\n",
      "[[-0.05198031]]\n",
      "0.01191501579098476\n",
      "\n",
      "\n",
      "[[ 0.01647767]]\n",
      "0.005813953488372098\n",
      "\n",
      "\n",
      "[[-0.00882711]]\n",
      "-0.026147186147186113\n",
      "\n",
      "\n",
      "[[ 0.03165936]]\n",
      "-0.06315789473684207\n",
      "\n",
      "\n",
      "[[-0.0311653]]\n",
      "0.054573574900574434\n",
      "\n",
      "\n",
      "[[-0.01856327]]\n",
      "-0.032954545454545556\n",
      "\n",
      "\n",
      "[[ 0.00049075]]\n",
      "0.011377010592389231\n",
      "\n",
      "\n",
      "[[ 0.02281527]]\n",
      "0.014583333333333393\n",
      "\n",
      "\n",
      "[[-0.01213127]]\n",
      "0.02774988589685081\n",
      "\n",
      "\n",
      "[[ 0.00961749]]\n",
      "-0.0015265056897030174\n",
      "\n",
      "\n",
      "[[ 0.06719172]]\n",
      "0.23171628077912532\n",
      "\n",
      "\n",
      "[[ 0.00719043]]\n",
      "-0.07192532942898974\n",
      "\n",
      "\n",
      "[[-0.00011088]]\n",
      "-0.001430274135876097\n",
      "\n",
      "\n",
      "[[ 0.02645759]]\n",
      "-0.02682926829268279\n",
      "\n",
      "\n",
      "[[ 0.01348867]]\n",
      "0.0020254304039607584\n",
      "\n",
      "\n",
      "[[ 0.01915139]]\n",
      "0.18461538461538465\n",
      "\n",
      "\n",
      "[[-0.01113451]]\n",
      "-0.11456628477905073\n",
      "\n",
      "\n",
      "[[-0.0408384]]\n",
      "0.013120899718837783\n",
      "\n",
      "\n",
      "[[ 0.08445547]]\n",
      "0.013119533527696915\n",
      "\n",
      "\n",
      "[[-0.05162455]]\n",
      "-0.03112338858195208\n",
      "\n",
      "\n",
      "[[-0.06387571]]\n",
      "-0.03156708004509579\n",
      "\n",
      "\n",
      "[[ 0.02012231]]\n",
      "-0.010710583899573795\n",
      "\n",
      "\n",
      "[[ 0.03673698]]\n",
      "0.012557077625570711\n",
      "\n",
      "\n",
      "[[ 0.17049986]]\n",
      "0.01572327044025146\n",
      "\n",
      "\n",
      "[[ 0.00420393]]\n",
      "0.04509722796855605\n",
      "\n",
      "\n",
      "[[ 0.0049918]]\n",
      "0.11498257839721257\n",
      "\n",
      "\n",
      "[[-0.01237301]]\n",
      "-0.03453870281573344\n",
      "\n",
      "\n",
      "[[-0.06144514]]\n",
      "-0.012999518536350485\n",
      "\n",
      "\n",
      "[[ 0.02202419]]\n",
      "0.014609301702184739\n",
      "\n",
      "\n",
      "[[-0.00829747]]\n",
      "-0.031101739588824454\n",
      "\n",
      "\n",
      "[[ 0.00212318]]\n",
      "-0.012979209229659876\n",
      "\n",
      "\n",
      "[[-0.02194323]]\n",
      "0.02123322575165619\n",
      "\n",
      "\n",
      "[[-0.06492711]]\n",
      "-0.0022932857688876622\n",
      "\n",
      "\n",
      "[[ 0.05026304]]\n",
      "0.1388199503642623\n",
      "\n",
      "\n",
      "[[-0.00327173]]\n",
      "0.019579879371304728\n",
      "\n",
      "\n",
      "[[-0.06364937]]\n",
      "-0.02470671880554569\n",
      "\n",
      "\n",
      "[[ 0.01391741]]\n",
      "-0.01886792452830182\n",
      "\n",
      "\n",
      "[[-0.01366125]]\n",
      "0.06249999999999992\n",
      "\n",
      "\n",
      "[[ 0.02178154]]\n",
      "-0.0028449502133713065\n",
      "\n",
      "\n",
      "[[ 0.04903457]]\n",
      "0.043659747292418824\n",
      "\n",
      "\n",
      "[[-0.06014381]]\n",
      "-0.006163886874546706\n",
      "\n",
      "\n",
      "[[-0.08382277]]\n",
      "0.011391907817205582\n",
      "\n",
      "\n",
      "[[ 0.00102424]]\n",
      "-0.0033719704952582443\n",
      "\n",
      "\n",
      "[[-0.08019563]]\n",
      "-0.02104499274310589\n",
      "\n",
      "\n",
      "[[-0.00190537]]\n",
      "0.061410788381742604\n",
      "\n",
      "\n",
      "[[ 0.0526767]]\n",
      "0.01920709184929823\n",
      "\n",
      "\n",
      "[[-0.04891764]]\n",
      "0.006677796327211997\n",
      "\n",
      "\n",
      "[[ 0.03607856]]\n",
      "-0.007385524372230428\n",
      "\n",
      "\n",
      "[[ 0.03017403]]\n",
      "0.001515151515151429\n",
      "\n",
      "\n",
      "[[ 0.04728033]]\n",
      "0.004810326980454366\n",
      "\n",
      "\n",
      "[[ 0.00443219]]\n",
      "0.026701767581797792\n",
      "\n",
      "\n",
      "[[-0.02709167]]\n",
      "-0.03347732181425491\n",
      "\n",
      "\n",
      "[[-0.01920339]]\n",
      "0.024570583551359305\n",
      "\n",
      "\n",
      "[[-0.05490243]]\n",
      "0.0\n",
      "\n",
      "\n",
      "[[-0.02858365]]\n",
      "-0.013789452455379719\n",
      "\n",
      "\n",
      "[[ 0.02003091]]\n",
      "0.006993006993007093\n",
      "\n",
      "\n",
      "[[ 0.0690309]]\n",
      "-0.004206098843322853\n",
      "\n",
      "\n",
      "[[ 0.04237798]]\n",
      "-0.057034220532319393\n",
      "\n",
      "\n",
      "[[ 0.00205903]]\n",
      "0.03565692258271753\n",
      "\n",
      "\n",
      "[[-0.02312548]]\n",
      "-0.040719101123595516\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X)):\n",
    "    print(m1.predict(np.array([X[i]])))\n",
    "    print(y[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
