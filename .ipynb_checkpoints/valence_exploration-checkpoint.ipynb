{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_path = 'features_by_valence/'\n",
    "label_set = ['low', 'high']\n",
    "low_files = glob.glob(root_path + label_set[0] + '/*')\n",
    "high_files = glob.glob(root_path + label_set[1] + '/*')\n",
    "\n",
    "features = {'low':[], 'high':[]}\n",
    "\n",
    "all_features = []\n",
    "\n",
    "for filename in low_files:\n",
    "    word_histogram = json.loads(open(filename).read())\n",
    "    features['low'].append(word_histogram)\n",
    "    all_features.append({'dictionary': word_histogram, 'label': 'low'})\n",
    "for filename in high_files:\n",
    "    word_histogram = json.loads(open(filename).read())\n",
    "    features['high'].append(word_histogram)\n",
    "    all_features.append({'dictionary': word_histogram, 'label': 'high'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total: 252\n",
      "train: 201\n",
      "test: 50\n"
     ]
    }
   ],
   "source": [
    "n_total = int(len(all_features))\n",
    "n_train = int(n_total * 0.8)\n",
    "n_test = int(n_total * 0.2)\n",
    "\n",
    "print('total: ' + str(n_total))\n",
    "print('train: ' + str(n_train))\n",
    "print('test: ' + str(n_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'high': 109, 'low': 143}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = {}\n",
    "label_counts['high'] = len(features['high'])\n",
    "label_counts['low'] = len(features['low'])\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'high': 1, 'low': 0}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one hot encoding\n",
    "ohe = {}\n",
    "i_2_label = {}\n",
    "i_2_label[0] = 'low'\n",
    "i_2_label[1] = 'high'\n",
    "ohe['high'] = 1\n",
    "ohe['low'] = 0\n",
    "\n",
    "ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "def train_word_embeddings(af):\n",
    "    wes = {}\n",
    "    for d_a_l_o in af[0:n_train]:\n",
    "        dictionary = d_a_l_o['dictionary']\n",
    "        label = d_a_l_o['label']\n",
    "        for word in dictionary:\n",
    "            word_count_in_document = dictionary[word]\n",
    "            wes[word] = wes.get(word, np.zeros(len(ohe)))\n",
    "            pos = ohe[label]\n",
    "            wes[word][pos] = wes[word][pos] + (float(word_count_in_document)/float(label_counts[label]))\n",
    "    return wes\n",
    "\n",
    "wes = train_word_embeddings(all_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 229.18181818,  117.1559633 ])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wes['certain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(dictionary, wes):\n",
    "    outcome = np.zeros(len(ohe))\n",
    "    for word in dictionary:\n",
    "        lookup = wes.get( word, np.zeros(len(ohe)) )\n",
    "        norm_lookup = float(np.linalg.norm(lookup))\n",
    "        for i, n in enumerate(lookup):\n",
    "            label_count = label_counts[i_2_label[int(i)]]\n",
    "            outcome[i] = outcome[i] + n/(norm_lookup*label_count)\n",
    "    pi = np.argmax(outcome)\n",
    "    oi = np.argmin(outcome)\n",
    "    return [i_2_label[pi], (outcome[pi]/outcome[oi])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low 1.44699573209\n"
     ]
    }
   ],
   "source": [
    "classification, score = classify(features['high'][11], wes)\n",
    "print(classification, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "accurate_surety = []\n",
    "inaccurate_surety = []\n",
    "def validate(af, wes):\n",
    "    correct_preds = 0\n",
    "    all_preds = 0\n",
    "    for d_a_l_o in af[n_train:n_total]:\n",
    "        dictionary = d_a_l_o['dictionary']\n",
    "        label = d_a_l_o['label']\n",
    "        classification, score = classify(dictionary, wes)\n",
    "        if classification == label:\n",
    "            accurate_surety.append(score)\n",
    "            correct_preds = correct_preds + 1\n",
    "        else:\n",
    "            inaccurate_surety.append(score)\n",
    "            \n",
    "        all_preds = all_preds + 1\n",
    "    return [float(correct_preds)/float(all_preds), np.average(accurate_surety), np.average(inaccurate_surety)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validating:  51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Piper/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6274509803921569, nan, nan]\n",
      "validating:  51\n",
      "[0.5882352941176471, nan, nan]\n"
     ]
    }
   ],
   "source": [
    "def find_acc(data):\n",
    "    for i in range(2):\n",
    "        shuffle(all_features)\n",
    "        wes = train_word_embeddings(all_features)\n",
    "        print(validate(all_features, wes))\n",
    "        \n",
    "find_acc(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
